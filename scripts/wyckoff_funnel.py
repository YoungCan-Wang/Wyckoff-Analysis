# -*- coding: utf-8 -*-
"""
Wyckoff Funnel å®šæ—¶ä»»åŠ¡ï¼š4 å±‚æ¼æ–—ç­›é€‰ â†’ é£ä¹¦å‘é€

Layer 1: å‰¥ç¦»åƒåœ¾ â†’ Layer 2: å¼ºå¼±ç”„åˆ« â†’ Layer 3: æ¿å—å…±æŒ¯ â†’ Layer 4: å¨ç§‘å¤«ç‹™å‡»
"""
from __future__ import annotations
import os
import socket
import sys
import time
from concurrent.futures import ProcessPoolExecutor, TimeoutError as FuturesTimeoutError, as_completed
from datetime import date, datetime, timedelta
from zoneinfo import ZoneInfo

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd

from integrations.fetch_a_share_csv import (
    _resolve_trading_window,
    get_stocks_by_board,
    _normalize_symbols,
)
from core.wyckoff_engine import (
    FunnelConfig,
    layer1_filter,
    layer2_strength,
    layer3_sector_resonance,
    layer4_triggers,
    normalize_hist_from_fetch,
)
from integrations.data_source import fetch_index_hist, fetch_sector_map, fetch_market_cap_map
from utils.feishu import send_feishu_notification

TRIGGER_LABELS = {
    "spring": "Springï¼ˆç»ˆæéœ‡ä»“ï¼‰",
    "lps": "LPSï¼ˆç¼©é‡å›è¸©ï¼‰",
    "evr": "Effort vs Resultï¼ˆæ”¾é‡ä¸è·Œï¼‰",
}
TRADING_DAYS = 500
MAX_RETRIES = int(os.getenv("FUNNEL_FETCH_RETRIES", "2"))
RETRY_BASE_DELAY = float(os.getenv("FUNNEL_RETRY_BASE_DELAY", "1.0"))
SOCKET_TIMEOUT = int(os.getenv("FUNNEL_SOCKET_TIMEOUT", "20"))
FETCH_TIMEOUT = int(os.getenv("FUNNEL_FETCH_TIMEOUT", "45"))
BATCH_TIMEOUT = int(os.getenv("FUNNEL_BATCH_TIMEOUT", "420"))
BATCH_SIZE = int(os.getenv("FUNNEL_BATCH_SIZE", "200"))
BATCH_SLEEP = float(os.getenv("FUNNEL_BATCH_SLEEP", "2"))
MAX_WORKERS = int(os.getenv("FUNNEL_MAX_WORKERS", "8"))
CN_TZ = ZoneInfo("Asia/Shanghai")
MARKET_CLOSE_HOUR = int(os.getenv("MARKET_CLOSE_HOUR", "15"))


def _normalize_hist(df: pd.DataFrame) -> pd.DataFrame:
    return normalize_hist_from_fetch(df)


def _fetch_hist(symbol: str, window, adjust: str) -> pd.DataFrame:
    from integrations.fetch_a_share_csv import _fetch_hist as _fh
    df = _fh(symbol=symbol, window=window, adjust=adjust)
    return _normalize_hist(df)


def _stock_name_map() -> dict[str, str]:
    try:
        from integrations.fetch_a_share_csv import get_all_stocks
        items = get_all_stocks()
        return {x.get("code", ""): x.get("name", "") for x in items if isinstance(x, dict)}
    except Exception:
        return {}


def _fetch_one_with_retry(sym: str, window, max_retries: int = MAX_RETRIES) -> tuple[str, pd.DataFrame | None]:
    """åœ¨å­è¿›ç¨‹ä¸­æ‰§è¡Œï¼Œå•ç¥¨ç¡¬è¶…æ—¶ + é‡è¯•ï¼Œé¿å…ä¸ªåˆ«æ•°æ®æºå¡æ­»æ‹–æ…¢æ•´æ‰¹ã€‚"""
    socket.setdefaulttimeout(SOCKET_TIMEOUT)
    for attempt in range(max_retries):
        try:
            df = _run_with_timeout(sym, window, FETCH_TIMEOUT)
            return (sym, df)
        except Exception:
            if attempt < max_retries - 1:
                delay = RETRY_BASE_DELAY * (attempt + 1)
                time.sleep(delay)
    return (sym, None)


def _run_with_timeout(sym: str, window, timeout_s: int) -> pd.DataFrame:
    """
    åœ¨ worker è¿›ç¨‹å†…ç»™å•ç¥¨è¯·æ±‚åŠ ç¡¬è¶…æ—¶ï¼ˆUnix ä¸‹ç”¨ SIGALRMï¼‰ã€‚
    è‹¥å¹³å°ä¸æ”¯æŒ SIGALRMï¼ˆä¾‹å¦‚ Windowsï¼‰ï¼Œåˆ™é€€åŒ–ä¸ºç›´æ¥è°ƒç”¨ã€‚
    æ³¨æ„ï¼šåœ¨ Windows / ä¸æ”¯æŒ SIGALRM çš„è¿è¡Œç¯å¢ƒé‡Œï¼Œæœ¬å‡½æ•°ä¸ä¼šæä¾›å•ç¥¨ç¡¬è¶…æ—¶ï¼Œ
    ä»…ä¾èµ–å¤–å±‚æ‰¹æ¬¡è¶…æ—¶(BATCH_TIMEOUT)åšå…œåº•ã€‚
    """
    if timeout_s <= 0:
        return _fetch_hist(sym, window, "qfq")

    try:
        import signal
    except Exception:
        return _fetch_hist(sym, window, "qfq")

    if not hasattr(signal, "SIGALRM"):
        return _fetch_hist(sym, window, "qfq")

    def _alarm_handler(signum, frame):  # pragma: no cover - signal handler
        raise TimeoutError(f"single fetch timeout>{timeout_s}s")

    old = signal.signal(signal.SIGALRM, _alarm_handler)
    signal.alarm(timeout_s)
    try:
        return _fetch_hist(sym, window, "qfq")
    finally:
        signal.alarm(0)
        signal.signal(signal.SIGALRM, old)


def _job_end_calendar_day() -> date:
    """
    å®šæ—¶ä»»åŠ¡ç»Ÿä¸€å£å¾„ï¼š
    - åŒ—äº¬æ—¶é—´æ”¶ç›˜åï¼ˆé»˜è®¤ >=15:00ï¼‰èµ° T+0ï¼ˆå½“å¤©ï¼‰
    - æ”¶ç›˜å‰èµ° T-1ï¼ˆä¸Šä¸€è‡ªç„¶æ—¥ï¼‰
    """
    now = datetime.now(CN_TZ)
    if now.hour >= MARKET_CLOSE_HOUR:
        return now.date()
    return (now - timedelta(days=1)).date()


def _terminate_executor_processes(ex: ProcessPoolExecutor, batch_no: int) -> None:
    """
    æ‰¹æ¬¡è¶…æ—¶æ—¶ï¼Œä¸»åŠ¨ç»ˆæ­¢ä»å­˜æ´»çš„å­è¿›ç¨‹ï¼Œé¿å… wait=False ä»…â€œé€»è¾‘ç»“æŸâ€ä½†è¿›ç¨‹ç»§ç»­è·‘ã€‚
    è¿™é‡Œä½¿ç”¨ç§æœ‰å±æ€§æ˜¯å‡ºäºç¨³å®šæ€§æƒè¡¡ï¼šè¯¥ä»»åŠ¡æ›´çœ‹é‡ç¡¬è¶…æ—¶æ­¢æŸã€‚
    """
    procs = getattr(ex, "_processes", {}) or {}
    killed = 0
    for proc in procs.values():
        try:
            if proc.is_alive():
                proc.terminate()
                proc.join(timeout=1)
                if proc.is_alive():
                    proc.kill()
                    proc.join(timeout=1)
                killed += 1
        except Exception as e:
            print(f"[funnel] æ‰¹æ¬¡#{batch_no} ç»ˆæ­¢å­è¿›ç¨‹å¼‚å¸¸: {e}")
    if killed:
        print(f"[funnel] æ‰¹æ¬¡#{batch_no} å·²å¼ºåˆ¶ç»ˆæ­¢ {killed} ä¸ªå¡ä½å­è¿›ç¨‹")


def _analyze_benchmark_and_tune_cfg(
    bench_df: pd.DataFrame | None,
    cfg: FunnelConfig,
) -> dict:
    """
    Step 0ï¼šå¤§ç›˜æ€»é—¸
    - è¾“å‡ºå®è§‚æ°´æ¸©ï¼ˆRISK_ON / NEUTRAL / RISK_OFFï¼‰
    - åœ¨ RISK_OFF æ—¶åŠ¨æ€æ”¶ç´§ä¸ªè‚¡è¿‡æ»¤é˜ˆå€¼
    """
    context = {
        "regime": "UNKNOWN",
        "close": None,
        "ma50": None,
        "ma200": None,
        "ma50_slope_5d": None,
        "recent3_pct": [],
        "recent3_cum_pct": None,
        "tuned": {
            "min_avg_amount_wan": cfg.min_avg_amount_wan,
            "rs_min_long": cfg.rs_min_long,
            "rs_min_short": cfg.rs_min_short,
        },
    }
    if bench_df is None or bench_df.empty:
        return context

    b = bench_df.sort_values("date").copy()
    b["close"] = pd.to_numeric(b["close"], errors="coerce")
    b["pct_chg"] = pd.to_numeric(b["pct_chg"], errors="coerce")
    if len(b) < 60:
        return context

    close = float(b["close"].iloc[-1])
    ma50 = float(b["close"].rolling(50).mean().iloc[-1])
    ma200 = float(b["close"].rolling(200).mean().iloc[-1])
    ma50_prev = b["close"].rolling(50).mean().shift(5).iloc[-1]
    ma50_slope_5d = None if pd.isna(ma50_prev) else float(ma50 - ma50_prev)
    recent3 = b["pct_chg"].dropna().tail(3)
    recent3_list = [float(x) for x in recent3.tolist()]
    recent3_cum = None
    if not recent3.empty:
        recent3_cum = float(((recent3 / 100.0 + 1.0).prod() - 1.0) * 100.0)

    regime = "NEUTRAL"
    if (
        pd.notna(ma200)
        and pd.notna(ma50)
        and ma50_slope_5d is not None
        and recent3_cum is not None
    ):
        risk_off = (close < ma200) and (ma50 < ma200) and (ma50_slope_5d < 0) and (recent3_cum <= -2.0)
        risk_on = (close > ma50 > ma200) and (ma50_slope_5d > 0) and (recent3_cum >= 0.0)
        if risk_off:
            regime = "RISK_OFF"
        elif risk_on:
            regime = "RISK_ON"

    # åŠ¨æ€è°ƒå‚ï¼šé£é™©è¶Šå†·ï¼Œè¿‡æ»¤è¶Šä¸¥
    if regime == "RISK_OFF":
        cfg.min_avg_amount_wan = max(cfg.min_avg_amount_wan, 10000.0)
        cfg.rs_min_long = max(cfg.rs_min_long, 2.0)
        cfg.rs_min_short = max(cfg.rs_min_short, 0.5)
        if recent3_cum is not None and recent3_cum <= -4.0:
            cfg.min_avg_amount_wan = max(cfg.min_avg_amount_wan, 15000.0)
            cfg.rs_min_long = max(cfg.rs_min_long, 4.0)
            cfg.rs_min_short = max(cfg.rs_min_short, 1.0)
    elif regime == "RISK_ON":
        cfg.rs_min_long = max(cfg.rs_min_long, 0.0)
        cfg.rs_min_short = max(cfg.rs_min_short, 0.0)

    context.update({
        "regime": regime,
        "close": close,
        "ma50": ma50,
        "ma200": ma200,
        "ma50_slope_5d": ma50_slope_5d,
        "recent3_pct": recent3_list,
        "recent3_cum_pct": recent3_cum,
        "tuned": {
            "min_avg_amount_wan": cfg.min_avg_amount_wan,
            "rs_min_long": cfg.rs_min_long,
            "rs_min_short": cfg.rs_min_short,
        },
    })
    return context


def run_funnel_job() -> tuple[dict[str, list[tuple[str, float]]], dict]:
    """æ‰§è¡Œ Wyckoff Funnelï¼Œè¿”å› (triggers, metrics)ã€‚"""
    cfg = FunnelConfig(trading_days=TRADING_DAYS)
    window = _resolve_trading_window(
        end_calendar_day=_job_end_calendar_day(),
        trading_days=TRADING_DAYS,
    )
    start_s = window.start_trade_date.strftime("%Y%m%d")
    end_s = window.end_trade_date.strftime("%Y%m%d")

    # è‚¡ç¥¨æ± ï¼šä¸»æ¿ + åˆ›ä¸šæ¿ - STï¼ˆé¢„è¿‡æ»¤ï¼Œå‡å°‘æ— æ•ˆæ‹‰å–ï¼‰
    main_items = get_stocks_by_board("main")
    chinext_items = get_stocks_by_board("chinext")
    merged_code_to_name: dict[str, str] = {}
    for item in main_items + chinext_items:
        code = str(item.get("code", "")).strip()
        if not code:
            continue
        if code not in merged_code_to_name:
            merged_code_to_name[code] = str(item.get("name", "")).strip()
    merged_symbols = _normalize_symbols(list(merged_code_to_name.keys()))
    st_symbols = [
        sym for sym in merged_symbols
        if "ST" in merged_code_to_name.get(sym, "").upper()
    ]
    st_set = set(st_symbols)
    all_symbols = [sym for sym in merged_symbols if sym not in st_set]
    total_batches = (len(all_symbols) + BATCH_SIZE - 1) // BATCH_SIZE if all_symbols else 0
    print(
        "[funnel] è‚¡ç¥¨æ± ç»Ÿè®¡: "
        f"main={len(main_items)}, chinext={len(chinext_items)}, "
        f"merged={len(merged_symbols)}, st_excluded={len(st_symbols)}, "
        f"final={len(all_symbols)}, batches={total_batches} (batch_size={BATCH_SIZE})"
    )

    # æ‰¹é‡å…ƒæ•°æ®
    print(f"[funnel] åŠ è½½è¡Œä¸šæ˜ å°„...")
    sector_map = fetch_sector_map()
    print(f"[funnel] åŠ è½½å¸‚å€¼æ•°æ®...")
    market_cap_map = fetch_market_cap_map()
    if not market_cap_map:
        print("[funnel] âš ï¸ å¸‚å€¼æ•°æ®ä¸ºç©ºï¼ˆTUSHARE_TOKEN å¯èƒ½ç¼ºå¤±/å¤±æ•ˆï¼‰ï¼ŒLayer1 å°†è·³è¿‡å¸‚å€¼è¿‡æ»¤")
    print(f"[funnel] åŠ è½½è‚¡ç¥¨åç§°...")
    name_map = _stock_name_map()

    # å¤§ç›˜åŸºå‡†
    bench_df = None
    try:
        bench_df = fetch_index_hist("000001", start_s, end_s)
        print(f"[funnel] å¤§ç›˜åŸºå‡†åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"[funnel] å¤§ç›˜åŸºå‡†åŠ è½½å¤±è´¥: {e}")

    # Step 0: å¤§ç›˜æ€»é—¸ + åŠ¨æ€é˜ˆå€¼
    benchmark_context = _analyze_benchmark_and_tune_cfg(bench_df, cfg)
    print(
        "[funnel] å¤§ç›˜æ€»é—¸: "
        f"regime={benchmark_context['regime']}, "
        f"close={benchmark_context['close']}, ma50={benchmark_context['ma50']}, ma200={benchmark_context['ma200']}, "
        f"ma50_slope_5d={benchmark_context['ma50_slope_5d']}, recent3={benchmark_context['recent3_pct']}, "
        f"recent3_cum={benchmark_context['recent3_cum_pct']}, "
        f"tuned={benchmark_context['tuned']}"
    )

    # å¹¶å‘æ‹‰å–æ—¥çº¿ï¼ˆåªè´Ÿè´£å–æ•°ï¼Œä¸è´Ÿè´£è®¡ç®—ï¼‰
    all_df_map: dict[str, pd.DataFrame] = {}
    fetch_ok = 0
    fetch_fail = 0

    print(
        f"[funnel] å¼€å§‹æ‹‰å– {len(all_symbols)} åªè‚¡ç¥¨æ—¥çº¿ "
        f"(batch_size={BATCH_SIZE}, max_workers={MAX_WORKERS}, batch_timeout={BATCH_TIMEOUT}s, "
        f"fetch_timeout={FETCH_TIMEOUT}s, retries={MAX_RETRIES})"
    )
    for i in range(0, len(all_symbols), BATCH_SIZE):
        batch_no = i // BATCH_SIZE + 1
        batch = all_symbols[i: i + BATCH_SIZE]
        batch_ok = 0
        batch_fail = 0
        batch_started = time.monotonic()
        print(f"[funnel] æ‰¹æ¬¡#{batch_no}/{total_batches} å¯åŠ¨ï¼Œè‚¡ç¥¨æ•°={len(batch)}")

        ex = ProcessPoolExecutor(max_workers=MAX_WORKERS)
        futures = {ex.submit(_fetch_one_with_retry, s, window): s for s in batch}
        try:
            for f in as_completed(futures, timeout=BATCH_TIMEOUT):
                sym = futures[f]
                try:
                    _, df = f.result()
                except Exception as e:
                    print(f"[funnel] æ‰¹æ¬¡#{batch_no} æ‹‰å–å¤±è´¥ {sym}: {e}")
                    batch_fail += 1
                    fetch_fail += 1
                    continue
                if df is not None:
                    batch_ok += 1
                    fetch_ok += 1
                    all_df_map[sym] = df
                else:
                    batch_fail += 1
                    fetch_fail += 1
        except FuturesTimeoutError:
            pending_symbols = [futures[ft] for ft in futures if not ft.done()]
            timed_out = len(pending_symbols)
            batch_fail += timed_out
            fetch_fail += timed_out
            print(
                f"[funnel] æ‰¹æ¬¡#{batch_no} è¶…æ—¶({BATCH_TIMEOUT}s)ï¼Œ"
                f"å·²å®Œæˆ={batch_ok + batch_fail - timed_out}/{len(batch)}ï¼Œ"
                f"æœªå®Œæˆ={timed_out}ï¼Œå°†è·³è¿‡å‰©ä½™ä»»åŠ¡"
            )
            if pending_symbols:
                preview = ", ".join(pending_symbols[:10])
                suffix = "..." if len(pending_symbols) > 10 else ""
                print(f"[funnel] æ‰¹æ¬¡#{batch_no} è¶…æ—¶è‚¡ç¥¨: {preview}{suffix}")
            _terminate_executor_processes(ex, batch_no)
        finally:
            for ft in futures:
                ft.cancel()
            ex.shutdown(wait=False, cancel_futures=True)

        batch_elapsed = time.monotonic() - batch_started
        print(
            f"[funnel] æ‰¹æ¬¡#{batch_no} å®Œæˆ: æˆåŠŸ={batch_ok}, å¤±è´¥={batch_fail}, "
            f"è€—æ—¶={batch_elapsed:.1f}s, ç´¯è®¡æˆåŠŸ={fetch_ok}, ç´¯è®¡å¤±è´¥={fetch_fail}"
        )
        if i + BATCH_SIZE < len(all_symbols) and BATCH_SLEEP > 0:
            time.sleep(BATCH_SLEEP)

    print(f"[funnel] æ—¥çº¿æ‹‰å–å®Œæˆ: æˆåŠŸ={fetch_ok}, å¤±è´¥={fetch_fail}")

    # ç»Ÿä¸€æ¼æ–—è®¡ç®—ï¼šL1 -> L2 -> L3 -> L4
    print(f"[funnel] å¼€å§‹æ‰§è¡Œå…¨é‡æ¼æ–—ç­›é€‰...")
    
    # Layer 1
    l1_input = list(all_df_map.keys())
    l1_passed = layer1_filter(l1_input, name_map, market_cap_map, all_df_map, cfg)
    
    # Layer 2
    l2_passed = layer2_strength(l1_passed, all_df_map, bench_df, cfg)
    
    # Layer 3 (Sector Resonance)
    l3_passed, top_sectors = layer3_sector_resonance(l2_passed, sector_map, cfg)
    
    # Layer 4 (Wyckoff Triggers)
    # L4 éœ€è¦ l2_df_mapï¼Œè¿™é‡Œç›´æ¥ç”¨ all_df_map å³å¯ï¼Œå› ä¸º key éƒ½åœ¨é‡Œé¢
    triggers = layer4_triggers(l3_passed, all_df_map, cfg)
    
    total_hits = sum(len(v) for v in triggers.values())
    metrics = {
        "total_symbols": len(all_symbols),
        "pool_main": len(main_items),
        "pool_chinext": len(chinext_items),
        "pool_merged": len(merged_symbols),
        "pool_st_excluded": len(st_symbols),
        "pool_batches": total_batches,
        "fetch_ok": fetch_ok,
        "fetch_fail": fetch_fail,
        "layer1": len(l1_passed),
        "layer2": len(l2_passed),
        "layer3": len(l3_passed),
        "top_sectors": top_sectors,
        "total_hits": total_hits,
        "by_trigger": {k: len(v) for k, v in triggers.items()},
        "benchmark_context": benchmark_context,
    }
    print(f"[funnel] L1={metrics['layer1']}, L2={metrics['layer2']}, "
          f"L3={metrics['layer3']}, å‘½ä¸­={total_hits}, "
          f"Topè¡Œä¸š={top_sectors}, å„è§¦å‘={metrics['by_trigger']}")
    return triggers, metrics


def run(webhook_url: str) -> tuple[bool, list[dict], dict]:
    """
    æ‰§è¡Œ Wyckoff Funnelï¼Œæ¼æ–—å®Œæˆåç«‹å³å‘é€é£ä¹¦é€šçŸ¥ã€‚
    è¿”å› (æˆåŠŸä¸å¦, ç”¨äºç ”æŠ¥çš„è‚¡ç¥¨ä¿¡æ¯åˆ—è¡¨, å¤§ç›˜ä¸Šä¸‹æ–‡)ã€‚
    æ¯é¡¹ä¸º {"code": str, "name": str, "tag": str}ã€‚
    """
    triggers, metrics = run_funnel_job()
    benchmark_context = metrics.get("benchmark_context", {}) or {}
    name_map = _stock_name_map()

    code_to_reasons: dict[str, list[str]] = {}
    code_to_best_score: dict[str, float] = {}
    for key, label in TRIGGER_LABELS.items():
        for code, score in triggers.get(key, []):
            if code not in code_to_reasons:
                code_to_reasons[code] = []
                code_to_best_score[code] = score
            code_to_reasons[code].append(label)
            code_to_best_score[code] = max(code_to_best_score.get(code, 0), score)

    sorted_codes = sorted(
        code_to_reasons.keys(),
        key=lambda c: -code_to_best_score.get(c, 0),
    )
    unique_hit_count = len(sorted_codes)
    selected_for_ai = sorted_codes

    print(
        f"[funnel] å€™é€‰åˆ†å±‚: å‘½ä¸­äº‹ä»¶={metrics['total_hits']}, å‘½ä¸­è‚¡ç¥¨={unique_hit_count}, "
        f"AIè¾“å…¥=å…¨é‡{len(selected_for_ai)}, "
        f"AIåˆ†æ={len(selected_for_ai)}"
    )

    bench_line = "æœªçŸ¥"
    if benchmark_context:
        bench_line = (
            f"{benchmark_context.get('regime')} | close={benchmark_context.get('close')} "
            f"ma50={benchmark_context.get('ma50')} ma200={benchmark_context.get('ma200')} "
            f"3d={benchmark_context.get('recent3_pct')} cum3={benchmark_context.get('recent3_cum_pct')}"
        )

    lines = [
        (
            f"**è‚¡ç¥¨æ± **: ä¸»æ¿{metrics['pool_main']} + åˆ›ä¸šæ¿{metrics['pool_chinext']} "
            f"-> å»é‡{metrics['pool_merged']} -> å»ST{metrics['pool_st_excluded']} "
            f"= {metrics['total_symbols']} (å…±{metrics['pool_batches']}æ‰¹)"
        ),
        f"**æ¼æ–—æ¦‚è§ˆ**: {metrics['total_symbols']}åª â†’ L1:{metrics['layer1']} â†’ L2:{metrics['layer2']} â†’ L3:{metrics['layer3']} â†’ å‘½ä¸­:{metrics['total_hits']}",
        f"**å¤§ç›˜æ°´æ¸©**: {bench_line}",
        f"**å€™é€‰åˆ†å±‚**: å‘½ä¸­è‚¡ç¥¨{unique_hit_count} -> AIè¾“å…¥å…¨é‡{len(selected_for_ai)}",
        f"**Top è¡Œä¸š**: {', '.join(metrics['top_sectors']) if metrics['top_sectors'] else 'æ— '}",
        "",
        "**å‘½ä¸­åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ä»£ç  åç§° | ç­›é€‰ç†ç”± | åˆ†å€¼**",
        "",
    ]
    for code in selected_for_ai:
        name = name_map.get(code, code)
        reasons = "ã€".join(code_to_reasons[code])
        lines.append(f"â€¢ {code} {name} | {reasons} | score={code_to_best_score.get(code, 0):.2f}")

    if not selected_for_ai:
        lines.append("æ— ")

    content = "\n".join(lines)
    title = f"ğŸ”¬ Wyckoff Funnel {date.today().strftime('%Y-%m-%d')}"
    ok = send_feishu_notification(webhook_url, title, content)

    symbols_for_report = [
        {
            "code": c,
            "name": name_map.get(c, c),
            "tag": "ã€".join(code_to_reasons[c]),
        }
        for c in selected_for_ai
    ]
    return (ok, symbols_for_report, benchmark_context)
