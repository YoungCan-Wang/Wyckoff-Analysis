# -*- coding: utf-8 -*-
"""
Wyckoff Funnel å®šæ—¶ä»»åŠ¡ï¼š4 å±‚æ¼æ–—ç­›é€‰ â†’ é£ä¹¦å‘é€

Layer 1: å‰¥ç¦»åƒåœ¾ â†’ Layer 2: å¼ºå¼±ç”„åˆ« â†’ Layer 3: æ¿å—å…±æŒ¯ â†’ Layer 4: å¨ç§‘å¤«ç‹™å‡»
"""
from __future__ import annotations

import os
import socket
import sys
import time
from concurrent.futures import ProcessPoolExecutor, as_completed
from datetime import date, timedelta

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd

from fetch_a_share_csv import (
    _resolve_trading_window,
    get_stocks_by_board,
    _normalize_symbols,
)
from wyckoff_engine import (
    FunnelConfig,
    layer1_filter,
    layer2_strength,
    layer3_sector_resonance,
    layer4_triggers,
    normalize_hist_from_fetch,
)
from data_source import fetch_index_hist, fetch_sector_map, fetch_market_cap_map
from utils.feishu import send_feishu_notification

TRIGGER_LABELS = {
    "spring": "Springï¼ˆç»ˆæéœ‡ä»“ï¼‰",
    "lps": "LPSï¼ˆç¼©é‡å›è¸©ï¼‰",
    "evr": "Effort vs Resultï¼ˆæ”¾é‡ä¸è·Œï¼‰",
}
TRADING_DAYS = 500
MAX_RETRIES = 3
RETRY_BASE_DELAY = 2
SOCKET_TIMEOUT = 30
BATCH_TIMEOUT = 600
BATCH_SIZE = 200
BATCH_SLEEP = 5
MAX_WORKERS = 8
STEP3_MAX_SYMBOLS = 6


def _normalize_hist(df: pd.DataFrame) -> pd.DataFrame:
    return normalize_hist_from_fetch(df)


def _fetch_hist(symbol: str, window, adjust: str) -> pd.DataFrame:
    from fetch_a_share_csv import _fetch_hist as _fh
    df = _fh(symbol=symbol, window=window, adjust=adjust)
    return _normalize_hist(df)


def _stock_name_map() -> dict[str, str]:
    try:
        from fetch_a_share_csv import get_all_stocks
        items = get_all_stocks()
        return {x.get("code", ""): x.get("name", "") for x in items if isinstance(x, dict)}
    except Exception:
        return {}


def _fetch_one_with_retry(sym: str, window, max_retries: int = MAX_RETRIES) -> tuple[str, pd.DataFrame | None]:
    """åœ¨å­è¿›ç¨‹ä¸­æ‰§è¡Œï¼Œè¿›ç¨‹çº§ socket è¶…æ—¶ä¸å½±å“ä¸»è¿›ç¨‹ã€‚"""
    socket.setdefaulttimeout(SOCKET_TIMEOUT)
    for attempt in range(max_retries):
        try:
            df = _fetch_hist(sym, window, "qfq")
            return (sym, df)
        except Exception:
            if attempt < max_retries - 1:
                delay = RETRY_BASE_DELAY * (2 ** attempt)
                time.sleep(delay)
    return (sym, None)


def _terminate_executor_processes(ex: ProcessPoolExecutor, batch_no: int) -> None:
    """
    æ‰¹æ¬¡è¶…æ—¶æ—¶ï¼Œä¸»åŠ¨ç»ˆæ­¢ä»å­˜æ´»çš„å­è¿›ç¨‹ï¼Œé¿å… wait=False ä»…â€œé€»è¾‘ç»“æŸâ€ä½†è¿›ç¨‹ç»§ç»­è·‘ã€‚
    è¿™é‡Œä½¿ç”¨ç§æœ‰å±æ€§æ˜¯å‡ºäºç¨³å®šæ€§æƒè¡¡ï¼šè¯¥ä»»åŠ¡æ›´çœ‹é‡ç¡¬è¶…æ—¶æ­¢æŸã€‚
    """
    procs = getattr(ex, "_processes", {}) or {}
    killed = 0
    for proc in procs.values():
        try:
            if proc.is_alive():
                proc.terminate()
                proc.join(timeout=1)
                if proc.is_alive():
                    proc.kill()
                    proc.join(timeout=1)
                killed += 1
        except Exception as e:
            print(f"[funnel] æ‰¹æ¬¡#{batch_no} ç»ˆæ­¢å­è¿›ç¨‹å¼‚å¸¸: {e}")
    if killed:
        print(f"[funnel] æ‰¹æ¬¡#{batch_no} å·²å¼ºåˆ¶ç»ˆæ­¢ {killed} ä¸ªå¡ä½å­è¿›ç¨‹")


def run_funnel_job() -> tuple[dict[str, list[tuple[str, float]]], dict]:
    """æ‰§è¡Œ Wyckoff Funnelï¼Œè¿”å› (triggers, metrics)ã€‚"""
    cfg = FunnelConfig(trading_days=TRADING_DAYS)
    window = _resolve_trading_window(
        end_calendar_day=date.today() - timedelta(days=1),
        trading_days=TRADING_DAYS,
    )
    start_s = window.start_trade_date.strftime("%Y%m%d")
    end_s = window.end_trade_date.strftime("%Y%m%d")

    # è‚¡ç¥¨æ± ï¼šä¸»æ¿ + åˆ›ä¸šæ¿ - STï¼ˆé¢„è¿‡æ»¤ï¼Œå‡å°‘æ— æ•ˆæ‹‰å–ï¼‰
    main_items = get_stocks_by_board("main")
    chinext_items = get_stocks_by_board("chinext")
    merged_code_to_name: dict[str, str] = {}
    for item in main_items + chinext_items:
        code = str(item.get("code", "")).strip()
        if not code:
            continue
        if code not in merged_code_to_name:
            merged_code_to_name[code] = str(item.get("name", "")).strip()
    merged_symbols = _normalize_symbols(list(merged_code_to_name.keys()))
    st_symbols = [
        sym for sym in merged_symbols
        if "ST" in merged_code_to_name.get(sym, "").upper()
    ]
    st_set = set(st_symbols)
    all_symbols = [sym for sym in merged_symbols if sym not in st_set]
    total_batches = (len(all_symbols) + BATCH_SIZE - 1) // BATCH_SIZE if all_symbols else 0
    print(
        "[funnel] è‚¡ç¥¨æ± ç»Ÿè®¡: "
        f"main={len(main_items)}, chinext={len(chinext_items)}, "
        f"merged={len(merged_symbols)}, st_excluded={len(st_symbols)}, "
        f"final={len(all_symbols)}, batches={total_batches} (batch_size={BATCH_SIZE})"
    )

    # æ‰¹é‡å…ƒæ•°æ®
    print(f"[funnel] åŠ è½½è¡Œä¸šæ˜ å°„...")
    sector_map = fetch_sector_map()
    print(f"[funnel] åŠ è½½å¸‚å€¼æ•°æ®...")
    market_cap_map = fetch_market_cap_map()
    if not market_cap_map:
        print("[funnel] âš ï¸ å¸‚å€¼æ•°æ®ä¸ºç©ºï¼ˆTUSHARE_TOKEN å¯èƒ½ç¼ºå¤±/å¤±æ•ˆï¼‰ï¼ŒLayer1 å°†è·³è¿‡å¸‚å€¼è¿‡æ»¤")
    print(f"[funnel] åŠ è½½è‚¡ç¥¨åç§°...")
    name_map = _stock_name_map()

    # å¤§ç›˜åŸºå‡†
    bench_df = None
    try:
        bench_df = fetch_index_hist("000001", start_s, end_s)
        print(f"[funnel] å¤§ç›˜åŸºå‡†åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"[funnel] å¤§ç›˜åŸºå‡†åŠ è½½å¤±è´¥: {e}")

    # å¹¶å‘æ‹‰å–æ—¥çº¿ + åˆ†æ‰¹æ¼æ–—ï¼ˆæ¯æ‰¹å…ˆåš L1/L2ï¼Œæœ€åç»Ÿä¸€åš L3/L4ï¼‰
    l1_passed_set: set[str] = set()
    l2_passed_set: set[str] = set()
    l2_df_map: dict[str, pd.DataFrame] = {}
    fetch_ok = 0
    fetch_fail = 0

    print(
        f"[funnel] å¼€å§‹æ‹‰å– {len(all_symbols)} åªè‚¡ç¥¨æ—¥çº¿ "
        f"(batch_size={BATCH_SIZE}, max_workers={MAX_WORKERS}, batch_timeout={BATCH_TIMEOUT}s)"
    )
    for i in range(0, len(all_symbols), BATCH_SIZE):
        batch_no = i // BATCH_SIZE + 1
        batch = all_symbols[i: i + BATCH_SIZE]
        batch_ok = 0
        batch_fail = 0
        batch_df_map: dict[str, pd.DataFrame] = {}
        batch_started = time.monotonic()
        print(f"[funnel] æ‰¹æ¬¡#{batch_no}/{total_batches} å¯åŠ¨ï¼Œè‚¡ç¥¨æ•°={len(batch)}")

        ex = ProcessPoolExecutor(max_workers=MAX_WORKERS)
        futures = {ex.submit(_fetch_one_with_retry, s, window): s for s in batch}
        try:
            for f in as_completed(futures, timeout=BATCH_TIMEOUT):
                sym = futures[f]
                try:
                    _, df = f.result()
                except Exception as e:
                    print(f"[funnel] æ‰¹æ¬¡#{batch_no} æ‹‰å–å¤±è´¥ {sym}: {e}")
                    batch_fail += 1
                    fetch_fail += 1
                    continue
                if df is not None:
                    batch_ok += 1
                    fetch_ok += 1
                    batch_df_map[sym] = df
                else:
                    batch_fail += 1
                    fetch_fail += 1
        except TimeoutError:
            pending_symbols = [futures[ft] for ft in futures if not ft.done()]
            timed_out = len(pending_symbols)
            batch_fail += timed_out
            fetch_fail += timed_out
            print(
                f"[funnel] æ‰¹æ¬¡#{batch_no} è¶…æ—¶({BATCH_TIMEOUT}s)ï¼Œ"
                f"å·²å®Œæˆ={batch_ok + batch_fail - timed_out}/{len(batch)}ï¼Œ"
                f"æœªå®Œæˆ={timed_out}ï¼Œå°†è·³è¿‡å‰©ä½™ä»»åŠ¡"
            )
            if pending_symbols:
                preview = ", ".join(pending_symbols[:10])
                suffix = "..." if len(pending_symbols) > 10 else ""
                print(f"[funnel] æ‰¹æ¬¡#{batch_no} è¶…æ—¶è‚¡ç¥¨: {preview}{suffix}")
            _terminate_executor_processes(ex, batch_no)
        finally:
            for ft in futures:
                ft.cancel()
            ex.shutdown(wait=False, cancel_futures=True)

        batch_l1 = layer1_filter(batch, name_map, market_cap_map, batch_df_map, cfg)
        batch_l2 = layer2_strength(batch_l1, batch_df_map, bench_df, cfg)
        l1_passed_set.update(batch_l1)
        l2_passed_set.update(batch_l2)
        for sym in batch_l2:
            df = batch_df_map.get(sym)
            if df is not None:
                l2_df_map[sym] = df

        batch_elapsed = time.monotonic() - batch_started
        print(
            f"[funnel] æ‰¹æ¬¡#{batch_no} å®Œæˆ: æˆåŠŸ={batch_ok}, å¤±è´¥={batch_fail}, "
            f"L1={len(batch_l1)}, L2={len(batch_l2)}, "
            f"è€—æ—¶={batch_elapsed:.1f}s, ç´¯è®¡æˆåŠŸ={fetch_ok}, ç´¯è®¡å¤±è´¥={fetch_fail}, "
            f"ç´¯è®¡L1={len(l1_passed_set)}, ç´¯è®¡L2={len(l2_passed_set)}"
        )
        if i + BATCH_SIZE < len(all_symbols) and BATCH_SLEEP > 0:
            time.sleep(BATCH_SLEEP)

    print(f"[funnel] æ—¥çº¿æ‹‰å–å®Œæˆ: æˆåŠŸ={fetch_ok}, å¤±è´¥={fetch_fail}")

    # æ±‡æ€»é˜¶æ®µï¼šå…¨å±€åš L3/L4ï¼ˆL3 ä¾èµ–å…¨å±€è¡Œä¸šåˆ†å¸ƒï¼‰
    l2_symbols = sorted(l2_passed_set)
    l3_symbols, top_sectors = layer3_sector_resonance(l2_symbols, sector_map, cfg)
    triggers = layer4_triggers(l3_symbols, l2_df_map, cfg)
    total_hits = sum(len(v) for v in triggers.values())
    metrics = {
        "total_symbols": len(all_symbols),
        "pool_main": len(main_items),
        "pool_chinext": len(chinext_items),
        "pool_merged": len(merged_symbols),
        "pool_st_excluded": len(st_symbols),
        "pool_batches": total_batches,
        "fetch_ok": fetch_ok,
        "fetch_fail": fetch_fail,
        "layer1": len(l1_passed_set),
        "layer2": len(l2_symbols),
        "layer3": len(l3_symbols),
        "top_sectors": top_sectors,
        "total_hits": total_hits,
        "by_trigger": {k: len(v) for k, v in triggers.items()},
    }
    print(f"[funnel] L1={metrics['layer1']}, L2={metrics['layer2']}, "
          f"L3={metrics['layer3']}, å‘½ä¸­={total_hits}, "
          f"Topè¡Œä¸š={top_sectors}, å„è§¦å‘={metrics['by_trigger']}")
    return triggers, metrics


def run(webhook_url: str) -> tuple[bool, list[dict]]:
    """
    æ‰§è¡Œ Wyckoff Funnelï¼Œæ¼æ–—å®Œæˆåç«‹å³å‘é€é£ä¹¦é€šçŸ¥ã€‚
    è¿”å› (æˆåŠŸä¸å¦, ç”¨äºç ”æŠ¥çš„è‚¡ç¥¨ä¿¡æ¯åˆ—è¡¨)ã€‚
    æ¯é¡¹ä¸º {"code": str, "name": str, "tag": str}ã€‚
    """
    triggers, metrics = run_funnel_job()
    name_map = _stock_name_map()

    code_to_reasons: dict[str, list[str]] = {}
    code_to_best_score: dict[str, float] = {}
    for key, label in TRIGGER_LABELS.items():
        for code, score in triggers.get(key, []):
            if code not in code_to_reasons:
                code_to_reasons[code] = []
                code_to_best_score[code] = score
            code_to_reasons[code].append(label)
            code_to_best_score[code] = max(code_to_best_score.get(code, 0), score)

    sorted_codes = sorted(
        code_to_reasons.keys(),
        key=lambda c: -code_to_best_score.get(c, 0),
    )

    lines = [
        (
            f"**è‚¡ç¥¨æ± **: ä¸»æ¿{metrics['pool_main']} + åˆ›ä¸šæ¿{metrics['pool_chinext']} "
            f"-> å»é‡{metrics['pool_merged']} -> å»ST{metrics['pool_st_excluded']} "
            f"= {metrics['total_symbols']} (å…±{metrics['pool_batches']}æ‰¹)"
        ),
        f"**æ¼æ–—æ¦‚è§ˆ**: {metrics['total_symbols']}åª â†’ L1:{metrics['layer1']} â†’ L2:{metrics['layer2']} â†’ L3:{metrics['layer3']} â†’ å‘½ä¸­:{metrics['total_hits']}",
        f"**Top è¡Œä¸š**: {', '.join(metrics['top_sectors']) if metrics['top_sectors'] else 'æ— '}",
        "",
        "**ç­›é€‰ç»“æœï¼ˆä»£ç  åç§° | ç­›é€‰ç†ç”±ï¼‰**",
        "",
    ]
    for code in sorted_codes:
        name = name_map.get(code, code)
        reasons = "ã€".join(code_to_reasons[code])
        lines.append(f"â€¢ {code} {name} | {reasons}")

    if not sorted_codes:
        lines.append("æ— ")

    content = "\n".join(lines)
    title = f"ğŸ”¬ Wyckoff Funnel {date.today().strftime('%Y-%m-%d')}"
    ok = send_feishu_notification(webhook_url, title, content)

    symbols_for_report = [
        {
            "code": c,
            "name": name_map.get(c, c),
            "tag": "ã€".join(code_to_reasons[c]),
        }
        for c in sorted_codes[:STEP3_MAX_SYMBOLS]
    ]
    return (ok, symbols_for_report)
