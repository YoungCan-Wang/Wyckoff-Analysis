# -*- coding: utf-8 -*-
"""
é˜¶æ®µ 3ï¼šæ‰¹é‡ AI ç ”æŠ¥
æ‹‰å–é€‰ä¸­è‚¡ç¥¨çš„ OHLCV â†’ ç¬¬äº”æ­¥ç‰¹å¾å·¥ç¨‹ â†’ AI åˆ†æ â†’ é£ä¹¦å‘é€
"""
from __future__ import annotations

import os
import re
import sys
from datetime import date, datetime, timedelta
from zoneinfo import ZoneInfo

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd

from integrations.ai_prompts import WYCKOFF_FUNNEL_SYSTEM_PROMPT
from integrations.fetch_a_share_csv import _resolve_trading_window, _fetch_hist
from integrations.llm_client import call_llm
from integrations.data_source import fetch_index_hist, fetch_sector_map
from utils.feishu import send_feishu_notification
from core.wyckoff_engine import normalize_hist_from_fetch

TRADING_DAYS = 500
GEMINI_MODEL_FALLBACK = "gemini-2.0-flash-lite"
OPERATION_TARGET = 6
STEP3_MAX_AI_INPUT = int(os.getenv("STEP3_MAX_AI_INPUT", "25"))
STEP3_MAX_PER_INDUSTRY = int(os.getenv("STEP3_MAX_PER_INDUSTRY", "5"))
STEP3_MAX_OUTPUT_TOKENS = 16384
DYNAMIC_MAINLINE_BONUS_RATE = 0.15
DYNAMIC_MAINLINE_TOP_N = 3
DYNAMIC_MAINLINE_MIN_CLUSTER = 2
STEP3_ENABLE_COMPRESSION = os.getenv("STEP3_ENABLE_COMPRESSION", "").strip().lower() in {
    "1", "true", "yes", "on"
}

RECENT_DAYS = 15
HIGHLIGHT_DAYS = 60
HIGHLIGHT_PCT_THRESHOLD = 5.0
HIGHLIGHT_VOL_RATIO = 2.0
DEBUG_MODEL_IO = os.getenv("DEBUG_MODEL_IO", "").strip().lower() in {"1", "true", "yes", "on"}
DEBUG_MODEL_IO_FULL = os.getenv("DEBUG_MODEL_IO_FULL", "").strip().lower() in {"1", "true", "yes", "on"}
CN_TZ = ZoneInfo("Asia/Shanghai")
MARKET_CLOSE_HOUR = int(os.getenv("MARKET_CLOSE_HOUR", "15"))


def _dump_model_input(
    items: list[dict],
    model: str,
    system_prompt: str,
    user_message: str,
) -> str:
    if not DEBUG_MODEL_IO:
        return ""

    logs_dir = os.getenv("LOGS_DIR", "logs")
    os.makedirs(logs_dir, exist_ok=True)
    path = os.path.join(logs_dir, f"step3_model_input_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")
    symbols_line = ", ".join(f"{x.get('code', '')}" for x in items)
    body = (
        f"[step3] model={model}\n"
        f"[step3] symbol_count={len(items)}\n"
        f"[step3] symbols={symbols_line}\n"
        f"[step3] system_prompt_len={len(system_prompt)}\n"
        f"[step3] user_message_len={len(user_message)}\n"
    )
    if DEBUG_MODEL_IO_FULL:
        body += (
            "\n===== SYSTEM PROMPT =====\n"
            f"{system_prompt}\n"
            "\n===== USER MESSAGE =====\n"
            f"{user_message}\n"
        )
    with open(path, "w", encoding="utf-8") as f:
        f.write(body)
    print(f"[step3] æ¨¡å‹è¾“å…¥å·²è½ç›˜: {path}")
    return path


def _has_required_sections(report: str) -> bool:
    text = (report or "").replace(" ", "")
    has_watch = ("è§‚å¯Ÿæ± " in text) or ("è‡ªé€‰è§‚å¯Ÿæ± " in text)
    has_trade = ("å¯æ“ä½œæ± " in text) or ("æ“ä½œæ± " in text)
    return has_watch and has_trade


def _repair_report_structure(
    report: str,
    model: str,
    api_key: str,
    selected_codes: list[str],
) -> str:
    """
    å½“æ¨¡å‹æœªç»™å‡ºâ€œè§‚å¯Ÿæ± /æ“ä½œæ± â€åŒå±‚ç»“æ„æ—¶ï¼Œåšä¸€æ¬¡ç»“æ„ä¿®å¤é‡å†™ã€‚
    """
    if not report.strip():
        return report

    repair_system = (
        "ä½ æ˜¯æ ¼å¼ä¿®å¤å™¨ã€‚è¯·å°†è¾“å…¥ç ”æŠ¥é‡æ’ä¸ºæ ‡å‡† Markdownï¼Œ"
        "å¿…é¡»åŒ…å«ä¸¤ä¸ªç« èŠ‚ï¼š1) è§‚å¯Ÿæ± ï¼ˆæ•°é‡ä¸é™ï¼Œå«è§‚å¯Ÿæ¡ä»¶ï¼‰"
        f" 2) å¯æ“ä½œæ± ï¼ˆå›ºå®š {OPERATION_TARGET} åªï¼Œè‹¥ä¸è¶³éœ€è¯´æ˜åŸå› ï¼‰ã€‚"
        "ä¸å¯æ–°å¢æœªåœ¨è¾“å…¥ä¸­å‡ºç°çš„è‚¡ç¥¨ä»£ç ã€‚"
    )
    repair_user = (
        "å…è®¸ä½¿ç”¨çš„è‚¡ç¥¨ä»£ç ï¼š"
        + ", ".join(selected_codes)
        + "\n\nä»¥ä¸‹æ˜¯å¾…ä¿®å¤æ–‡æœ¬ï¼š\n\n"
        + report
    )
    try:
        fixed = call_llm(
            provider="gemini",
            model=model,
            api_key=api_key,
            system_prompt=repair_system,
            user_message=repair_user,
            timeout=180,
            max_output_tokens=STEP3_MAX_OUTPUT_TOKENS,
        )
        return fixed or report
    except Exception as e:
        print(f"[step3] ç»“æ„ä¿®å¤å¤±è´¥: {e}")
        return report


def _build_fallback_sections(selected_df: pd.DataFrame) -> str:
    """
    æœ€åå…œåº•ï¼šç¡®ä¿é£ä¹¦ä¸€å®šå‡ºç°â€œè§‚å¯Ÿæ± /å¯æ“ä½œæ± â€ç»“æœå—ã€‚
    """
    if selected_df is None or selected_df.empty:
        return (
            "## ğŸ“š è§‚å¯Ÿæ± ï¼ˆç³»ç»Ÿå…œåº•ï¼‰\n"
            "- æœ¬è½®æ— å¯ç”¨å€™é€‰ã€‚\n\n"
            f"## âš”ï¸ å¯æ“ä½œæ± ï¼ˆç³»ç»Ÿå…œåº•ï¼Œç›®æ ‡ {OPERATION_TARGET} åªï¼‰\n"
            "- æœ¬è½®æ— å¯æ“ä½œæ ‡çš„ã€‚"
        )

    lines = ["## ğŸ“š è§‚å¯Ÿæ± ï¼ˆç³»ç»Ÿå…œåº•ï¼‰"]
    for _, row in selected_df.iterrows():
        code = str(row.get("code", ""))
        name = str(row.get("name", code))
        tag = str(row.get("tag", ""))
        score = row.get("wyckoff_score")
        score_text = f"{float(score):.3f}" if pd.notna(score) else "-"
        lines.append(
            f"- `{code} {name}` | æ ‡ç­¾: {tag or '-'} | é‡åŒ–åˆ†: {score_text} | è§‚å¯Ÿæ¡ä»¶: å›è¸©ç»“æ„æˆ˜åŒºæ—¶éœ€ç¼©é‡ç¡®è®¤ã€‚"
        )

    lines.append("")
    lines.append(f"## âš”ï¸ å¯æ“ä½œæ± ï¼ˆç³»ç»Ÿå…œåº•ï¼Œç›®æ ‡ {OPERATION_TARGET} åªï¼‰")
    top_ops = selected_df.head(OPERATION_TARGET)
    if top_ops.empty:
        lines.append("- æ— ")
    else:
        for _, row in top_ops.iterrows():
            code = str(row.get("code", ""))
            name = str(row.get("name", code))
            lines.append(
                f"- `{code} {name}` | æ¡ä»¶å»ºä»“: ä»…åœ¨æˆ˜åŒºå†…ç¼©é‡å›è¸©æˆ–å¼ºåŠ¿ç¡®è®¤å 1/3 è¯•å•ã€‚"
            )
    return "\n".join(lines)


def _extract_operation_pool_codes(report: str) -> list[str]:
    """
    ä»æ¨¡å‹æŠ¥å‘Šä¸­æå–â€œå¯æ“ä½œæ± /æ“ä½œæ± â€ç« èŠ‚å†…å‡ºç°çš„è‚¡ç¥¨ä»£ç ï¼ˆ6ä½ï¼‰ã€‚
    ç”¨äºå‘é€å‰ç½®é€Ÿè§ˆï¼Œé¿å…é•¿æ–‡ç¬¬ä¸€ç‰‡åªæ˜¾ç¤º1åªå¯¼è‡´é˜…è¯»ä¸å®Œæ•´ã€‚
    """
    if not report:
        return []
    lines = report.splitlines()
    in_ops = False
    codes: list[str] = []
    seen: set[str] = set()
    for raw in lines:
        line = raw.strip()
        if not line:
            continue
        if ("å¯æ“ä½œæ± " in line) or ("æ“ä½œæ± " in line):
            in_ops = True
            continue
        if (
            in_ops
            and line.startswith("## ")
            and ("å¯æ“ä½œæ± " not in line and "æ“ä½œæ± " not in line)
            and ("ğŸ¯" not in line)
        ):
            break
        if not in_ops:
            continue
        for c in re.findall(r"\b\d{6}\b", line):
            if c in seen:
                continue
            seen.add(c)
            codes.append(c)
    return codes


def _job_end_calendar_day() -> date:
    """
    å®šæ—¶ä»»åŠ¡ç»Ÿä¸€å£å¾„ï¼š
    - åŒ—äº¬æ—¶é—´æ”¶ç›˜åï¼ˆé»˜è®¤ >=15:00ï¼‰èµ° T+0ï¼ˆå½“å¤©ï¼‰
    - æ”¶ç›˜å‰èµ° T-1ï¼ˆä¸Šä¸€è‡ªç„¶æ—¥ï¼‰
    """
    now = datetime.now(CN_TZ)
    if now.hour >= MARKET_CLOSE_HOUR:
        return now.date()
    return (now - timedelta(days=1)).date()


def _safe_return(series: pd.Series, lookback: int = 10) -> float | None:
    s = pd.to_numeric(series, errors="coerce").dropna()
    if len(s) <= lookback:
        return None
    start = float(s.iloc[-lookback - 1])
    end = float(s.iloc[-1])
    if start == 0:
        return None
    return (end - start) / start * 100.0


def _resolve_bias_range(regime: str | None) -> tuple[float, float]:
    r = str(regime or "").upper()
    if r == "RISK_ON":
        return (-5.0, 45.0)
    if r == "RISK_OFF":
        return (0.0, 25.0)
    return (0.0, 35.0)


def _format_mainline_tag(industry: str | None, is_hot: bool) -> str:
    if not is_hot or not industry:
        return ""
    return f"ğŸ”¥ [å½“å‰èµ„é‡‘æœ€å¼ºä¸»çº¿: {industry}]"


def ultimate_compressor(
    candidates_df: pd.DataFrame,
    regime: str | None,
    bonus_rate: float = DYNAMIC_MAINLINE_BONUS_RATE,
    max_total: int = STEP3_MAX_AI_INPUT,
    max_per_industry: int = STEP3_MAX_PER_INDUSTRY,
) -> pd.DataFrame:
    """
    Step 4.5 ç»ˆæå‹ç¼©ï¼šåŠ¨æ€ä¹–ç¦»è¿‡æ»¤ + å› å­æ ‡å‡†åŒ– + åŠ¨æ€ä¸»çº¿è¯†åˆ« + è¡Œä¸šä¸Šé™ã€‚
    """
    if candidates_df is None or candidates_df.empty:
        return pd.DataFrame()

    df = candidates_df.copy()
    df["code"] = df.get("code", "").astype(str).str.strip()
    df["bias_200"] = pd.to_numeric(df.get("bias_200"), errors="coerce")
    df["rs_10"] = pd.to_numeric(df.get("rs_10"), errors="coerce")
    df["min_vol_ratio_5d"] = pd.to_numeric(df.get("min_vol_ratio_5d"), errors="coerce")
    df["industry"] = df.get("industry", "").astype(str).str.strip()
    df.loc[df["industry"] == "", "industry"] = pd.NA

    # å…ˆåˆ è„æ•°æ®ï¼šæ ¸å¿ƒå­—æ®µç¼ºå¤±ç›´æ¥æ·˜æ±°
    df = df.dropna(subset=["bias_200", "rs_10", "min_vol_ratio_5d", "industry"])
    if df.empty:
        return pd.DataFrame()

    # åŠ¨æ€æ°´æ¸©é˜ˆå€¼
    bias_min, bias_max = _resolve_bias_range(regime)
    df = df[(df["bias_200"] >= bias_min) & (df["bias_200"] <= bias_max)]
    if df.empty:
        return pd.DataFrame()

    # ç™¾åˆ†ä½å› å­åˆ†æ•°
    df["rs_score"] = df["rs_10"].rank(pct=True, ascending=True, method="average")
    # é‡æ¯”è¶Šå°è¶Šå¥½ï¼šascending=False ä½¿å°å€¼è·å¾—æ›´é«˜åˆ†ä½
    df["dry_score"] = df["min_vol_ratio_5d"].rank(
        pct=True, ascending=False, method="average"
    )
    df["base_wyckoff_score"] = 0.6 * df["rs_score"] + 0.4 * df["dry_score"]

    # åŠ¨æ€ä¸»çº¿è¯†åˆ«ï¼šå€™é€‰æ± å†…â€œæœ‰é›†ç¾¤ä¸”ç›¸å¯¹å¼ºåº¦é«˜â€çš„è¡Œä¸š
    industry_stats = (
        df.groupby("industry", as_index=False)
        .agg(stock_count=("code", "count"), avg_rs=("rs_score", "mean"))
    )
    valid_industry_stats = industry_stats[
        industry_stats["stock_count"] >= DYNAMIC_MAINLINE_MIN_CLUSTER
    ]
    hot_industries: set[str] = set()
    if not valid_industry_stats.empty:
        hot_industries = set(
            valid_industry_stats.nlargest(DYNAMIC_MAINLINE_TOP_N, "avg_rs")["industry"]
            .astype(str)
            .tolist()
        )
    df["is_hot_mainline"] = df["industry"].astype(str).isin(hot_industries)
    df["policy_tag"] = df.apply(
        lambda r: _format_mainline_tag(str(r.get("industry", "")), bool(r.get("is_hot_mainline"))),
        axis=1,
    )
    df["dynamic_bonus"] = df["is_hot_mainline"].map(
        lambda v: float(bonus_rate) if bool(v) else 0.0
    )
    df["wyckoff_score"] = df["base_wyckoff_score"] * (1.0 + df["dynamic_bonus"])

    # å…ˆå…¨å±€æ’åºï¼Œå†åšè¡Œä¸šæ‹¥æŒ¤åº¦é™åˆ¶
    df = df.sort_values("wyckoff_score", ascending=False).copy()
    df["industry_rank"] = (
        df.groupby("industry")["wyckoff_score"]
        .rank(ascending=False, method="first")
        .astype(int)
    )
    df = df.groupby("industry", group_keys=False).head(max_per_industry)
    df = df.head(max_total).reset_index(drop=True)
    if hot_industries:
        print(f"[step3] åŠ¨æ€ä¸»çº¿è¡Œä¸š: {', '.join(sorted(hot_industries))}")
    else:
        print("[step3] åŠ¨æ€ä¸»çº¿è¡Œä¸š: æ— ï¼ˆæœªå½¢æˆæœ‰æ•ˆè¡Œä¸šé›†ç¾¤ï¼‰")
    return df


def generate_stock_payload(
    stock_code: str,
    stock_name: str,
    wyckoff_tag: str,
    df: pd.DataFrame,
    *,
    industry: str | None = None,
    quant_score: float | None = None,
    industry_rank: int | None = None,
    policy_tag: str | None = None,
) -> str:
    """
    ç¬¬äº”æ­¥ï¼šå°† 500 å¤© OHLCV æµ“ç¼©ä¸ºå‘ç»™ AI çš„é«˜å¯†åº¦æ–‡æœ¬ã€‚
    1. å¤§èƒŒæ™¯ï¼ˆMA50 / MA200 / ä¹–ç¦»ç‡ï¼‰
    2. è¿‘ 15 æ—¥é‡ä»·åˆ‡ç‰‡ï¼ˆæ”¾é‡æ¯” + æ¶¨è·Œå¹…ï¼‰
    3. è¿‘ 60 æ—¥å¼‚åŠ¨é«˜å…‰æ—¶åˆ»
    """
    df = df.copy().sort_values("date").reset_index(drop=True)
    close = df["close"].astype(float)
    volume = df["volume"].astype(float)
    df["ma50"] = close.rolling(50).mean()
    df["ma200"] = close.rolling(200).mean()
    df["vol_ma20"] = volume.rolling(20).mean()
    df["pct_chg_calc"] = close.pct_change() * 100

    latest = df.iloc[-1]
    ma50_val = latest["ma50"]
    ma200_val = latest["ma200"]
    close_val = latest["close"]

    if pd.notna(ma50_val) and pd.notna(ma200_val) and ma200_val > 0:
        if ma50_val > ma200_val:
            trend = "é•¿æœŸå¤šå¤´æ’åˆ— (MA50 > MA200)"
        else:
            trend = "é•¿æœŸç©ºå¤´æˆ–éœ‡è¡ (MA50 <= MA200)"
        bias_200 = (close_val - ma200_val) / ma200_val * 100
        background = (
            f"  [ç»“æ„èƒŒæ™¯] ç°ä»·:{close_val:.2f}, MA50:{ma50_val:.2f}, MA200:{ma200_val:.2f}ã€‚"
            f"{trend}ï¼Œå¹´çº¿ä¹–ç¦»ç‡:{bias_200:.1f}%"
        )
    else:
        background = f"  [ç»“æ„èƒŒæ™¯] ç°ä»·:{close_val:.2f}ï¼ˆæ•°æ®ä¸è¶³ä»¥è®¡ç®— MA200ï¼‰"

    policy_prefix = f" {policy_tag}" if policy_tag else ""
    header = (
        f"â€¢ {stock_code} {stock_name}{policy_prefix} | æœºå™¨æ ‡ç­¾ï¼š{wyckoff_tag}\n"
        f"  [ä»·æ ¼é”šç‚¹] æœ€æ–°å®é™…æ”¶ç›˜ä»·={close_val:.2f}ï¼ˆæ‰§è¡Œå»ºè®®éœ€å›´ç»•è¯¥é”šç‚¹ç»™å‡ºç»“æ„æˆ˜åŒºï¼Œä¸å¾—ç»™å•ç‚¹é¢„æµ‹ä»·ï¼‰ã€‚\n"
        f"{background}\n"
    )
    if industry:
        header += f"  [è¡Œä¸š] {industry}\n"
    if quant_score is not None:
        rank_text = f"ï¼Œè¡Œä¸šå†…æ’å Top {industry_rank}" if industry_rank is not None else ""
        header += f"  [é‡åŒ–è¯„åˆ†] ç»¼åˆäººå› å­å¾—åˆ†: {quant_score:.3f}{rank_text}\n"

    # è¿‘ 15 æ—¥é‡ä»·åˆ‡ç‰‡
    recent = df.tail(RECENT_DAYS)
    recent_lines = ["  [è¿‘15æ—¥é‡ä»·åˆ‡ç‰‡]:"]
    for _, row in recent.iterrows():
        vol_ratio = row["volume"] / row["vol_ma20"] if pd.notna(row["vol_ma20"]) and row["vol_ma20"] > 0 else 0
        pct = row["pct_chg_calc"] if pd.notna(row["pct_chg_calc"]) else 0
        date_str = str(row["date"])[5:10]
        recent_lines.append(f"    {date_str}: æ”¶{row['close']:.2f} ({pct:+.1f}%), é‡æ¯”:{vol_ratio:.1f}x")

    # è¿‘ 60 æ—¥å¼‚åŠ¨é«˜å…‰
    tail60 = df.tail(HIGHLIGHT_DAYS)
    highlights = []
    for _, row in tail60.iterrows():
        pct = row["pct_chg_calc"] if pd.notna(row["pct_chg_calc"]) else 0
        vol_ratio = row["volume"] / row["vol_ma20"] if pd.notna(row["vol_ma20"]) and row["vol_ma20"] > 0 else 0
        if abs(pct) >= HIGHLIGHT_PCT_THRESHOLD or vol_ratio >= HIGHLIGHT_VOL_RATIO:
            date_str = str(row["date"])[5:10]
            tag_parts = []
            if abs(pct) >= HIGHLIGHT_PCT_THRESHOLD:
                tag_parts.append(f"æ¶¨è·Œ{pct:+.1f}%")
            if vol_ratio >= HIGHLIGHT_VOL_RATIO:
                tag_parts.append(f"é‡æ¯”{vol_ratio:.1f}x")
            highlights.append(f"    {date_str}: æ”¶{row['close']:.2f} ({', '.join(tag_parts)})")

    highlight_section = ""
    if highlights:
        highlight_section = "\n  [è¿‘60æ—¥å¼‚åŠ¨é«˜å…‰]:\n" + "\n".join(highlights) + "\n"

    return header + "\n".join(recent_lines) + "\n" + highlight_section + "\n"


def run(
    symbols_info: list[dict] | list[str],
    webhook_url: str,
    api_key: str,
    model: str,
    benchmark_context: dict | None = None,
) -> tuple[bool, str, str]:
    """
    æ‹‰å– OHLCV â†’ ç¬¬äº”æ­¥ç‰¹å¾å·¥ç¨‹ â†’ AI ç ”æŠ¥ â†’ é£ä¹¦å‘é€ã€‚
    symbols_info: list[{"code", "name", "tag"}] æˆ– list[str]ï¼ˆå‘åå…¼å®¹ï¼‰ã€‚
    """
    if not symbols_info:
        print("[step3] æ— è¾“å…¥è‚¡ç¥¨ï¼Œè·³è¿‡")
        return (True, "skipped_no_symbols", "")

    # å…¼å®¹æ—§è°ƒç”¨ï¼ˆçº¯ str åˆ—è¡¨ï¼‰
    items: list[dict] = []
    for s in symbols_info:
        if isinstance(s, str):
            items.append({"code": s, "name": s, "tag": ""})
        else:
            items.append(s)

    print(f"[step3] AI è¾“å…¥è‚¡ç¥¨æ•°={len(items)}ï¼ˆå…¨é‡å‘½ä¸­è¾“å…¥ï¼‰")

    end_day = _job_end_calendar_day()
    window = _resolve_trading_window(end_calendar_day=end_day, trading_days=TRADING_DAYS)

    regime = (benchmark_context or {}).get("regime", "NEUTRAL")
    sector_map = fetch_sector_map()
    benchmark_ret_10: float | None = None
    try:
        bench_df = fetch_index_hist("000001", window.start_trade_date, window.end_trade_date)
        benchmark_ret_10 = _safe_return(bench_df["close"], lookback=10)
    except Exception:
        benchmark_ret_10 = None

    parts: list[str] = []
    failed: list[tuple[str, str]] = []
    candidate_rows: list[dict] = []
    code_to_df: dict[str, pd.DataFrame] = {}
    for item in items:
        code = item["code"]
        name = item.get("name", code)
        tag = item.get("tag", "")
        try:
            df_raw = _fetch_hist(code, window, "qfq")
            df = normalize_hist_from_fetch(df_raw)
            code_to_df[code] = df

            close = pd.to_numeric(df["close"], errors="coerce")
            volume = pd.to_numeric(df["volume"], errors="coerce")
            ma200 = close.rolling(200).mean()
            latest_close = close.iloc[-1] if len(close) else pd.NA
            latest_ma200 = ma200.iloc[-1] if len(ma200) else pd.NA
            bias_200 = pd.NA
            if pd.notna(latest_close) and pd.notna(latest_ma200) and float(latest_ma200) != 0:
                bias_200 = (float(latest_close) - float(latest_ma200)) / float(latest_ma200) * 100.0

            stock_ret_10 = _safe_return(close, lookback=10)
            rs_10 = stock_ret_10
            if stock_ret_10 is not None and benchmark_ret_10 is not None:
                rs_10 = stock_ret_10 - benchmark_ret_10

            vol_ma20 = volume.rolling(20).mean()
            vol_ratio = volume / vol_ma20.replace(0, pd.NA)
            min_vol_ratio_5d = pd.to_numeric(vol_ratio.tail(5), errors="coerce").min()

            candidate_rows.append(
                {
                    "code": code,
                    "name": name,
                    "tag": tag,
                    "industry": sector_map.get(code, "æœªçŸ¥è¡Œä¸š"),
                    "bias_200": bias_200,
                    "rs_10": rs_10,
                    "min_vol_ratio_5d": min_vol_ratio_5d,
                }
            )
        except Exception as e:
            failed.append((code, str(e)))

    if not candidate_rows:
        if failed:
            detail = ", ".join(f"{s}({e})" for s, e in failed)
            print(f"[step3] OHLCV å…¨éƒ¨æ‹‰å–å¤±è´¥: {detail}")
            return (False, "data_all_failed", "")
        return (True, "no_data_but_no_error", "")

    candidates_df = pd.DataFrame(candidate_rows)
    candidates_df["code"] = candidates_df["code"].astype(str).str.strip()
    candidates_df["policy_tag"] = ""
    selected_df = candidates_df.copy()
    selected_df["wyckoff_score"] = pd.NA
    selected_df["industry_rank"] = pd.NA

    if STEP3_ENABLE_COMPRESSION:
        compressed_df = ultimate_compressor(
            candidates_df,
            regime=regime,
            bonus_rate=DYNAMIC_MAINLINE_BONUS_RATE,
            max_total=STEP3_MAX_AI_INPUT,
            max_per_industry=STEP3_MAX_PER_INDUSTRY,
        )
        if compressed_df.empty:
            print("[step3] å‹ç¼©å™¨ç»“æœä¸ºç©ºï¼Œå›é€€ä¸ºå…¨é‡å€™é€‰åˆ—è¡¨")
        else:
            selected_df = compressed_df
        print(
            f"[step3] å€™é€‰å‹ç¼©å·²å¯ç”¨: raw={len(candidates_df)} -> selected={len(selected_df)} "
            f"(regime={regime}, max_total={STEP3_MAX_AI_INPUT}, max_per_industry={STEP3_MAX_PER_INDUSTRY})"
        )
    else:
        print(f"[step3] å€™é€‰å‹ç¼©æœªå¯ç”¨: selected=å…¨é‡{len(selected_df)}")

    selected_codes = [str(x) for x in selected_df["code"].tolist()]
    for _, row in selected_df.iterrows():
        code = str(row["code"])
        df = code_to_df.get(code)
        if df is None:
            continue
        policy_val = row.get("policy_tag")
        policy_text = (
            str(policy_val).strip()
            if isinstance(policy_val, str) and str(policy_val).strip()
            else None
        )
        payload = generate_stock_payload(
            stock_code=code,
            stock_name=str(row.get("name", code)),
            wyckoff_tag=str(row.get("tag", "")),
            df=df,
            industry=str(row.get("industry", "")),
            quant_score=float(row["wyckoff_score"]) if pd.notna(row.get("wyckoff_score")) else None,
            industry_rank=int(row["industry_rank"]) if pd.notna(row.get("industry_rank")) else None,
            policy_tag=policy_text,
        )
        parts.append(payload)

    benchmark_lines = []
    if benchmark_context:
        benchmark_lines.append("[å®è§‚æ°´æ¸© / Benchmark Context]")
        benchmark_lines.append(
            f"regime={benchmark_context.get('regime')}, "
            f"close={benchmark_context.get('close')}, "
            f"ma50={benchmark_context.get('ma50')}, "
            f"ma200={benchmark_context.get('ma200')}, "
            f"ma50_slope_5d={benchmark_context.get('ma50_slope_5d')}"
        )
        benchmark_lines.append(
            f"recent3_pct={benchmark_context.get('recent3_pct')}, "
            f"recent3_cum_pct={benchmark_context.get('recent3_cum_pct')}, "
            f"tuned={benchmark_context.get('tuned')}"
        )

    user_message = (
        ("{}\n\n".format("\n".join(benchmark_lines)) if benchmark_lines else "")
        + (
            (
                f"[é‡åŒ–å‹ç¼©] å€™é€‰å·²ä» {len(candidates_df)} åªå‹ç¼©åˆ° {len(parts)} åªï¼Œ"
                f"regime={regime}, max_total={STEP3_MAX_AI_INPUT}, "
                f"max_per_industry={STEP3_MAX_PER_INDUSTRY}ã€‚\n\n"
            )
            if STEP3_ENABLE_COMPRESSION and len(candidates_df) > len(parts)
            else ""
        )
        + (
            "ä»¥ä¸‹æ˜¯é€šè¿‡ Wyckoff Funnel å‘½ä¸­å¹¶ç»é‡åŒ–å‹ç¼©åçš„å€™é€‰åå•ã€‚\n"
            if STEP3_ENABLE_COMPRESSION
            else "ä»¥ä¸‹æ˜¯é€šè¿‡ Wyckoff Funnel å‘½ä¸­çš„å…¨é‡å€™é€‰åå•ï¼ˆæœªå‹ç¼©ï¼‰ã€‚\n"
        )
        + "è¯·å…ˆä»å…¨éƒ¨è¾“å…¥ä¸­ç­›å‡ºâ€œå€¼å¾—åŠ å…¥è‡ªé€‰è§‚å¯Ÿæ± â€çš„æ ‡çš„ï¼ˆæ•°é‡ä¸é™ï¼‰ï¼Œå¹¶æ˜ç¡®æ¯åªçš„è§‚å¯Ÿæ¡ä»¶ï¼›"
        + f"å†ä»è§‚å¯Ÿæ± ä¸­ä¸¥æ ¼æŒ‘é€‰â€œæ¬¡æ—¥å¯ä¹°å…¥çš„æ“ä½œæ± â€{OPERATION_TARGET}åªã€‚\n"
        + f"è¾“å‡ºå¿…é¡»åŒ…å«ä¸¤ä¸ªéƒ¨åˆ†ï¼š1) è§‚å¯Ÿæ± ï¼ˆä¸é™ï¼Œå«è§‚å¯Ÿæ¡ä»¶ï¼‰ 2) æ“ä½œæ± ï¼ˆå›ºå®š{OPERATION_TARGET}åªï¼‰ã€‚\n"
        + "ç¡¬çº¦æŸï¼šæ“ä½œæ± å¿…é¡»æ˜¯è§‚å¯Ÿæ± å­é›†ï¼Œä¸”ä¸¤éƒ¨åˆ†åªèƒ½ä½¿ç”¨è¾“å…¥åˆ—è¡¨ä¸­çš„è‚¡ç¥¨ä»£ç ã€‚\n\n"
        + "äº¤æ˜“æ‰§è¡Œç¡¬çº¦æŸï¼š\n"
        + "1) ç¦æ­¢å•ç‚¹ä»·æ ¼æŒ‡ä»¤ï¼Œå¿…é¡»ç»™â€œç»“æ„æˆ˜åŒº(Action Zone) + ç›˜é¢ç¡®è®¤æ¡ä»¶(Tape Condition)â€ã€‚\n"
        + "2) æˆ˜åŒºéœ€å›´ç»•æ¯åªè‚¡ç¥¨çš„â€œä»·æ ¼é”šç‚¹ï¼ˆæœ€æ–°æ”¶ç›˜ä»·ï¼‰â€æè¿°ï¼Œä½†ä¸å¾—åˆ»èˆŸæ±‚å‰‘ã€‚\n"
        + "3) ä¹°å…¥è§¦å‘å¿…é¡»åŒ…å«é‡ä»·ç¡®è®¤æ¡ä»¶ï¼ˆå¦‚ç¼©é‡å›è¸©/æ‹’ç»ä¸‹ç ´ï¼‰ï¼›è‹¥æ”¾é‡ä¸‹ç ´ï¼Œå¿…é¡»å–æ¶ˆä¹°å…¥ã€‚\n"
        + "4) å¼ºåŠ¿çªç ´æ ‡çš„å¿…é¡»ç»™â€œé˜²è¸ç©ºç­–ç•¥â€ï¼šå¼€ç›˜å¼ºåŠ¿ç¡®è®¤åå¯å…ˆç”¨è®¡åˆ’ä»“ä½1/3è¯•å•ï¼Œå…¶ä½™ç­‰å¾…äºŒæ¬¡ç¡®è®¤ã€‚\n\n"
        + "\n".join(parts)
    )
    selected_set = set(selected_codes)
    selected_items = [x for x in items if str(x.get("code")) in selected_set]
    _dump_model_input(items=selected_items, model=model, system_prompt=WYCKOFF_FUNNEL_SYSTEM_PROMPT, user_message=user_message)

    report = ""
    used_model = ""
    models_to_try = [model]
    if GEMINI_MODEL_FALLBACK and GEMINI_MODEL_FALLBACK != model:
        models_to_try.append(GEMINI_MODEL_FALLBACK)

    for m in models_to_try:
        try:
            report = call_llm(
                provider="gemini",
                model=m,
                api_key=api_key,
                system_prompt=WYCKOFF_FUNNEL_SYSTEM_PROMPT,
                user_message=user_message,
                timeout=300,
                max_output_tokens=STEP3_MAX_OUTPUT_TOKENS,
            )
            used_model = m
            break
        except Exception as e:
            print(f"[step3] æ¨¡å‹ {m} å¤±è´¥: {e}")
            if m == models_to_try[-1]:
                return (False, "llm_failed", "")

    if not _has_required_sections(report):
        print("[step3] é¦–ç‰ˆç ”æŠ¥ç¼ºå°‘è§‚å¯Ÿæ± /å¯æ“ä½œæ± ï¼Œæ‰§è¡Œä¸€æ¬¡ç»“æ„ä¿®å¤")
        report = _repair_report_structure(
            report=report,
            model=used_model or model,
            api_key=api_key,
            selected_codes=selected_codes,
        )
    if not _has_required_sections(report):
        print("[step3] ç»“æ„ä¿®å¤åä»ç¼ºå°‘å…³é”®ç« èŠ‚ï¼Œè¿½åŠ ç³»ç»Ÿå…œåº•åˆ†å±‚")
        report = report.rstrip() + "\n\n" + _build_fallback_sections(selected_df)

    model_banner = f"ğŸ¤– æ¨¡å‹: {used_model or model}"
    code_name = {str(row.get("code")): str(row.get("name", row.get("code"))) for _, row in selected_df.iterrows()}
    ops_codes = _extract_operation_pool_codes(report)
    if len(ops_codes) < OPERATION_TARGET:
        # è‹¥æ¨¡å‹æ–‡æœ¬è¿‡é•¿æˆ–ç»“æ„å¼‚å¸¸å¯¼è‡´æå–ä¸è¶³ï¼Œè¡¥é½åˆ° 6 åªï¼Œä¿è¯ç¬¬ä¸€å±å¯è¯»
        for c in selected_codes:
            if c in ops_codes:
                continue
            ops_codes.append(c)
            if len(ops_codes) >= OPERATION_TARGET:
                break
    ops_lines = [f"- {c} {code_name.get(c, c)}" for c in ops_codes[:OPERATION_TARGET]]
    ops_preview = (
        "## âš”ï¸ å¯æ“ä½œæ± é€Ÿè§ˆï¼ˆå‰ç½®ï¼‰\n"
        + ("\n".join(ops_lines) if ops_lines else "- æ— ")
        + "\n\n---\n"
    )

    content = f"{model_banner}\n\n{ops_preview}\n{report}"
    print(f"[step3] é£ä¹¦å‘é€åŸæ–‡é•¿åº¦={len(content)}ï¼ˆä¸å‹ç¼©ï¼Œäº¤ç”±é£ä¹¦åˆ†ç‰‡ï¼‰")
    print(f"[step3] ç ”æŠ¥å®é™…ä½¿ç”¨æ¨¡å‹={used_model or model}")
    if failed:
        content += f"\n\n**è·å–å¤±è´¥**: {', '.join(f'{s}({e})' for s, e in failed)}"

    title = f"ğŸ“„ æ‰¹é‡ç ”æŠ¥ {date.today().strftime('%Y-%m-%d')}"
    sent = send_feishu_notification(webhook_url, title, content)
    if not sent:
        print("[step3] é£ä¹¦æ¨é€å¤±è´¥")
        return (False, "feishu_failed", report)
    print(f"[step3] ç ”æŠ¥å‘é€æˆåŠŸï¼Œè‚¡ç¥¨æ•°={len(parts)}ï¼Œæ‹‰å–å¤±è´¥æ•°={len(failed)}")
    return (True, "ok", report)
