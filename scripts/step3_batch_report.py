# -*- coding: utf-8 -*-
"""
é˜¶æ®µ 3ï¼šæ‰¹é‡ AI ç ”æŠ¥
æ‹‰å–é€‰ä¸­è‚¡ç¥¨çš„ OHLCV â†’ ç¬¬äº”æ­¥ç‰¹å¾å·¥ç¨‹ â†’ AI åˆ†æ â†’ é£ä¹¦å‘é€
"""
from __future__ import annotations

import json
import os
import re
import sys
import time
from datetime import date, datetime, timedelta
from zoneinfo import ZoneInfo

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd

from integrations.ai_prompts import WYCKOFF_FUNNEL_SYSTEM_PROMPT
from integrations.fetch_a_share_csv import _resolve_trading_window, _fetch_hist
from integrations.llm_client import call_llm
from integrations.rag_veto import is_rag_veto_enabled, run_negative_news_veto
from integrations.data_source import fetch_index_hist, fetch_sector_map, fetch_stock_spot_snapshot
from utils.feishu import send_feishu_notification
from core.wyckoff_engine import normalize_hist_from_fetch

TRADING_DAYS = 500
GEMINI_MODEL_FALLBACK = "gemini-2.0-flash-lite"
OPERATION_TARGET = 6
STEP3_MAX_AI_INPUT = int(os.getenv("STEP3_MAX_AI_INPUT", "25"))
STEP3_MAX_PER_INDUSTRY = int(os.getenv("STEP3_MAX_PER_INDUSTRY", "5"))
STEP3_MAX_OUTPUT_TOKENS = 16384
DYNAMIC_MAINLINE_BONUS_RATE = 0.15
DYNAMIC_MAINLINE_TOP_N = 3
DYNAMIC_MAINLINE_MIN_CLUSTER = 2
STEP3_ENABLE_COMPRESSION = os.getenv("STEP3_ENABLE_COMPRESSION", "1").strip().lower() in {
    "1", "true", "yes", "on"
}
STEP3_ENABLE_RAG_VETO = os.getenv("STEP3_ENABLE_RAG_VETO", "1").strip().lower() in {
    "1", "true", "yes", "on"
}


RECENT_DAYS = 15
HIGHLIGHT_DAYS = 60
HIGHLIGHT_PCT_THRESHOLD = 5.0
HIGHLIGHT_VOL_RATIO = 2.0
DEBUG_MODEL_IO = os.getenv("DEBUG_MODEL_IO", "").strip().lower() in {"1", "true", "yes", "on"}
DEBUG_MODEL_IO_FULL = os.getenv("DEBUG_MODEL_IO_FULL", "").strip().lower() in {"1", "true", "yes", "on"}
CN_TZ = ZoneInfo("Asia/Shanghai")
MARKET_CLOSE_HOUR = int(os.getenv("MARKET_CLOSE_HOUR", "15"))
MARKET_DATA_READY_HOUR = int(
    os.getenv(
        "MARKET_DATA_READY_HOUR",
        str(max(MARKET_CLOSE_HOUR, 20)),
    )
)
ENFORCE_TARGET_TRADE_DATE = os.getenv(
    "ENFORCE_TARGET_TRADE_DATE", "1"
).strip().lower() in {"1", "true", "yes", "on"}
STEP3_ENABLE_SPOT_PATCH = os.getenv("STEP3_ENABLE_SPOT_PATCH", "1").strip().lower() in {
    "1",
    "true",
    "yes",
    "on",
}
STEP3_SPOT_PATCH_RETRIES = int(os.getenv("STEP3_SPOT_PATCH_RETRIES", "2"))
STEP3_SPOT_PATCH_SLEEP = float(os.getenv("STEP3_SPOT_PATCH_SLEEP", "0.2"))


def _dump_model_input(
    items: list[dict],
    model: str,
    system_prompt: str,
    user_message: str,
) -> str:
    if not DEBUG_MODEL_IO:
        return ""

    logs_dir = os.getenv("LOGS_DIR", "logs")
    os.makedirs(logs_dir, exist_ok=True)
    path = os.path.join(logs_dir, f"step3_model_input_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")
    symbols_line = ", ".join(f"{x.get('code', '')}" for x in items)
    body = (
        f"[step3] model={model}\n"
        f"[step3] symbol_count={len(items)}\n"
        f"[step3] symbols={symbols_line}\n"
        f"[step3] system_prompt_len={len(system_prompt)}\n"
        f"[step3] user_message_len={len(user_message)}\n"
    )
    if DEBUG_MODEL_IO_FULL:
        body += (
            "\n===== SYSTEM PROMPT =====\n"
            f"{system_prompt}\n"
            "\n===== USER MESSAGE =====\n"
            f"{user_message}\n"
        )
    with open(path, "w", encoding="utf-8") as f:
        f.write(body)
    print(f"[step3] æ¨¡å‹è¾“å…¥å·²è½ç›˜: {path}")
    return path


def _has_required_sections(report: str) -> bool:
    text = (report or "").replace(" ", "")
    has_watch = ("è§‚å¯Ÿæ± " in text) or ("è‡ªé€‰è§‚å¯Ÿæ± " in text)
    has_trade = ("å¯æ“ä½œæ± " in text) or ("æ“ä½œæ± " in text)
    return has_watch and has_trade


def _repair_report_structure(
    report: str,
    model: str,
    api_key: str,
    selected_codes: list[str],
) -> str:
    """
    å½“æ¨¡å‹æœªç»™å‡ºâ€œè§‚å¯Ÿæ± /æ“ä½œæ± â€åŒå±‚ç»“æ„æ—¶ï¼Œåšä¸€æ¬¡ç»“æ„ä¿®å¤é‡å†™ã€‚
    """
    if not report.strip():
        return report

    repair_system = (
        "ä½ æ˜¯æ ¼å¼ä¿®å¤å™¨ã€‚è¯·å°†è¾“å…¥ç ”æŠ¥é‡æ’ä¸ºæ ‡å‡† Markdownï¼Œ"
        "å¿…é¡»åŒ…å«ä¸¤ä¸ªç« èŠ‚ï¼š1) è§‚å¯Ÿæ± ï¼ˆæ•°é‡ä¸é™ï¼Œå«è§‚å¯Ÿæ¡ä»¶ï¼‰"
        f" 2) å¯æ“ä½œæ± ï¼ˆå›ºå®š {OPERATION_TARGET} åªï¼Œè‹¥ä¸è¶³éœ€è¯´æ˜åŸå› ï¼‰ã€‚"
        "ä¸å¯æ–°å¢æœªåœ¨è¾“å…¥ä¸­å‡ºç°çš„è‚¡ç¥¨ä»£ç ã€‚"
    )
    repair_user = (
        "å…è®¸ä½¿ç”¨çš„è‚¡ç¥¨ä»£ç ï¼š"
        + ", ".join(selected_codes)
        + "\n\nä»¥ä¸‹æ˜¯å¾…ä¿®å¤æ–‡æœ¬ï¼š\n\n"
        + report
    )
    try:
        fixed = call_llm(
            provider="gemini",
            model=model,
            api_key=api_key,
            system_prompt=repair_system,
            user_message=repair_user,
            timeout=180,
            max_output_tokens=STEP3_MAX_OUTPUT_TOKENS,
        )
        return fixed or report
    except Exception as e:
        print(f"[step3] ç»“æ„ä¿®å¤å¤±è´¥: {e}")
        return report


def _build_fallback_sections(selected_df: pd.DataFrame) -> str:
    """
    æœ€åå…œåº•ï¼šç¡®ä¿é£ä¹¦ä¸€å®šå‡ºç°â€œè§‚å¯Ÿæ± /å¯æ“ä½œæ± â€ç»“æœå—ã€‚
    """
    if selected_df is None or selected_df.empty:
        return (
            "## ğŸ“š è§‚å¯Ÿæ± ï¼ˆç³»ç»Ÿå…œåº•ï¼‰\n"
            "- æœ¬è½®æ— å¯ç”¨å€™é€‰ã€‚\n\n"
            f"## âš”ï¸ å¯æ“ä½œæ± ï¼ˆç³»ç»Ÿå…œåº•ï¼Œç›®æ ‡ {OPERATION_TARGET} åªï¼‰\n"
            "- æœ¬è½®æ— å¯æ“ä½œæ ‡çš„ã€‚"
        )

    lines = ["## ğŸ“š è§‚å¯Ÿæ± ï¼ˆç³»ç»Ÿå…œåº•ï¼‰"]
    for _, row in selected_df.iterrows():
        code = str(row.get("code", ""))
        name = str(row.get("name", code))
        tag = str(row.get("tag", ""))
        score = row.get("wyckoff_score")
        score_text = f"{float(score):.3f}" if pd.notna(score) else "-"
        lines.append(
            f"- `{code} {name}` | æ ‡ç­¾: {tag or '-'} | é‡åŒ–åˆ†: {score_text} | è§‚å¯Ÿæ¡ä»¶: å›è¸©ç»“æ„æˆ˜åŒºæ—¶éœ€ç¼©é‡ç¡®è®¤ã€‚"
        )

    lines.append("")
    lines.append(f"## âš”ï¸ å¯æ“ä½œæ± ï¼ˆç³»ç»Ÿå…œåº•ï¼Œç›®æ ‡ {OPERATION_TARGET} åªï¼‰")
    top_ops = selected_df.head(OPERATION_TARGET)
    if top_ops.empty:
        lines.append("- æ— ")
    else:
        for _, row in top_ops.iterrows():
            code = str(row.get("code", ""))
            name = str(row.get("name", code))
            lines.append(
                f"- `{code} {name}` | æ¡ä»¶å»ºä»“: ä»…åœ¨æˆ˜åŒºå†…ç¼©é‡å›è¸©æˆ–å¼ºåŠ¿ç¡®è®¤å 1/3 è¯•å•ã€‚"
            )
    return "\n".join(lines)


def _extract_json_block(text: str) -> str:
    raw = (text or "").strip()
    if raw.startswith("```"):
        raw = re.sub(r"^```(?:json)?\s*", "", raw, flags=re.IGNORECASE)
        raw = re.sub(r"\s*```$", "", raw)
    start = raw.find("{")
    end = raw.rfind("}")
    if start >= 0 and end > start:
        return raw[start : end + 1]
    return raw


def _normalize_structured_pool(
    payload: dict,
    allowed_codes: set[str],
    code_name: dict[str, str],
) -> dict[str, list[dict[str, str]]]:
    watch_raw = (
        payload.get("watch_pool")
        or payload.get("observation_pool")
        or payload.get("watchlist")
        or payload.get("è§‚å¯Ÿæ± ")
        or []
    )
    ops_raw = (
        payload.get("operation_pool")
        or payload.get("tradable_pool")
        or payload.get("æ“ä½œæ± ")
        or payload.get("å¯æ“ä½œæ± ")
        or []
    )

    watch_items: list[dict[str, str]] = []
    op_items: list[dict[str, str]] = []
    seen_watch: set[str] = set()
    seen_ops: set[str] = set()

    if isinstance(watch_raw, list):
        for item in watch_raw:
            if not isinstance(item, dict):
                continue
            code = str(item.get("code", "")).strip()
            if not re.fullmatch(r"\d{6}", code) or code not in allowed_codes:
                continue
            if code in seen_watch:
                continue
            seen_watch.add(code)
            watch_items.append(
                {
                    "code": code,
                    "name": str(item.get("name", "")).strip() or code_name.get(code, code),
                    "reason": str(item.get("reason", "")).strip(),
                    "condition": str(item.get("condition", "")).strip(),
                }
            )

    if isinstance(ops_raw, list):
        for item in ops_raw:
            if not isinstance(item, dict):
                continue
            code = str(item.get("code", "")).strip()
            if not re.fullmatch(r"\d{6}", code) or code not in allowed_codes:
                continue
            if code in seen_ops:
                continue
            seen_ops.add(code)
            op_items.append(
                {
                    "code": code,
                    "name": str(item.get("name", "")).strip() or code_name.get(code, code),
                    "action": str(item.get("action", "")).strip(),
                    "reason": str(item.get("reason", "")).strip(),
                    "entry_condition": str(item.get("entry_condition", "")).strip(),
                }
            )

    return {
        "watch_pool": watch_items,
        "operation_pool": op_items,
    }


def _try_parse_structured_report(
    report: str,
    allowed_codes: set[str],
    code_name: dict[str, str],
) -> dict[str, list[dict[str, str]]] | None:
    raw = (report or "").strip()
    if not raw:
        return None
    for candidate in [raw, _extract_json_block(raw)]:
        if not candidate:
            continue
        try:
            payload = json.loads(candidate)
        except Exception:
            continue
        if not isinstance(payload, dict):
            continue
        normalized = _normalize_structured_pool(payload, allowed_codes, code_name)
        if normalized["watch_pool"] or normalized["operation_pool"]:
            return normalized
    return None





def _extract_codes_from_text(
    text: str,
    allowed_codes: set[str],
) -> list[str]:
    codes: list[str] = []
    seen: set[str] = set()
    for code in re.findall(r"\b\d{6}\b", text or ""):
        if code not in allowed_codes or code in seen:
            continue
        seen.add(code)
        codes.append(code)
    return codes


def _job_end_calendar_day() -> date:
    """
    å®šæ—¶ä»»åŠ¡ç»Ÿä¸€å£å¾„ï¼š
    - åŒ—äº¬æ—¶é—´è¾¾åˆ° MARKET_DATA_READY_HOURï¼ˆé»˜è®¤ >=20:00ï¼‰èµ° T+0ï¼ˆå½“å¤©ï¼‰
    - å¦åˆ™èµ° T-1ï¼ˆä¸Šä¸€è‡ªç„¶æ—¥ï¼‰ï¼Œé¿å… 15:00~18:00 æ•°æ®æºæ—¶å·®å¯¼è‡´æˆªé¢é”™ä½
    """
    now = datetime.now(CN_TZ)
    if now.hour >= MARKET_DATA_READY_HOUR:
        return now.date()
    return (now - timedelta(days=1)).date()


def _latest_trade_date_from_hist(df: pd.DataFrame) -> date | None:
    if df is None or df.empty or "date" not in df.columns:
        return None
    s = pd.to_datetime(df["date"], errors="coerce").dropna()
    if s.empty:
        return None
    return s.iloc[-1].date()


def _append_spot_bar_if_needed(
    code: str,
    df: pd.DataFrame,
    target_trade_date: date,
) -> tuple[pd.DataFrame, bool]:
    if not STEP3_ENABLE_SPOT_PATCH or df is None or df.empty:
        return (df, False)
    latest_trade_date = _latest_trade_date_from_hist(df)
    if latest_trade_date is None or latest_trade_date >= target_trade_date:
        return (df, False)
    if target_trade_date != datetime.now(CN_TZ).date():
        return (df, False)

    df_s = df.sort_values("date").reset_index(drop=True)
    last_close_series = pd.to_numeric(df_s.get("close"), errors="coerce").dropna()
    prev_close = float(last_close_series.iloc[-1]) if not last_close_series.empty else None

    for attempt in range(max(STEP3_SPOT_PATCH_RETRIES, 1)):
        snap = fetch_stock_spot_snapshot(code, force_refresh=attempt > 0)
        close_v = None if not snap else snap.get("close")
        if close_v is None or float(close_v) <= 0:
            if attempt < max(STEP3_SPOT_PATCH_RETRIES, 1) - 1:
                time.sleep(max(STEP3_SPOT_PATCH_SLEEP, 0.0))
            continue

        close_f = float(close_v)
        open_f = float(snap.get("open")) if snap and snap.get("open") is not None else close_f
        high_raw = float(snap.get("high")) if snap and snap.get("high") is not None else close_f
        low_raw = float(snap.get("low")) if snap and snap.get("low") is not None else close_f
        high_f = max(high_raw, open_f, close_f)
        low_f = min(low_raw, open_f, close_f)
        volume_f = float(snap.get("volume")) if snap and snap.get("volume") is not None else 0.0
        amount_f = float(snap.get("amount")) if snap and snap.get("amount") is not None else 0.0
        pct_f = float(snap.get("pct_chg")) if snap and snap.get("pct_chg") is not None else None
        if pct_f is None and prev_close and prev_close > 0:
            pct_f = (close_f - prev_close) / prev_close * 100.0

        new_row = {
            "date": target_trade_date.isoformat(),
            "open": open_f,
            "high": high_f,
            "low": low_f,
            "close": close_f,
            "volume": volume_f,
            "amount": amount_f,
            "pct_chg": pct_f if pct_f is not None else 0.0,
        }
        patched = pd.concat([df_s, pd.DataFrame([new_row])], ignore_index=True)
        patched = patched.sort_values("date").reset_index(drop=True)
        return (patched, True)
    return (df, False)


def _safe_return(series: pd.Series, lookback: int = 10) -> float | None:
    s = pd.to_numeric(series, errors="coerce").dropna()
    if len(s) <= lookback:
        return None
    start = float(s.iloc[-lookback - 1])
    end = float(s.iloc[-1])
    if start == 0:
        return None
    return (end - start) / start * 100.0


def _resolve_bias_range(regime: str | None) -> tuple[float, float]:
    r = str(regime or "").upper()
    if r == "RISK_ON":
        return (-5.0, 45.0)
    if r == "RISK_OFF":
        return (0.0, 25.0)
    return (0.0, 35.0)


def _format_mainline_tag(industry: str | None, is_hot: bool) -> str:
    if not is_hot or not industry:
        return ""
    return f"ğŸ”¥ [å½“å‰èµ„é‡‘æœ€å¼ºä¸»çº¿: {industry}]"


def ultimate_compressor(
    candidates_df: pd.DataFrame,
    regime: str | None,
    bonus_rate: float = DYNAMIC_MAINLINE_BONUS_RATE,
    max_total: int = STEP3_MAX_AI_INPUT,
    max_per_industry: int = STEP3_MAX_PER_INDUSTRY,
) -> pd.DataFrame:
    """
    Step 4.5 ç»ˆæå‹ç¼©ï¼šåŠ¨æ€ä¹–ç¦»è¿‡æ»¤ + å› å­æ ‡å‡†åŒ– + åŠ¨æ€ä¸»çº¿è¯†åˆ« + è¡Œä¸šä¸Šé™ã€‚
    """
    if candidates_df is None or candidates_df.empty:
        return pd.DataFrame()

    df = candidates_df.copy()
    df["code"] = df.get("code", "").astype(str).str.strip()
    df["bias_200"] = pd.to_numeric(df.get("bias_200"), errors="coerce")
    df["rs_10"] = pd.to_numeric(df.get("rs_10"), errors="coerce")
    df["min_vol_ratio_5d"] = pd.to_numeric(df.get("min_vol_ratio_5d"), errors="coerce")
    df["industry"] = df.get("industry", "").astype(str).str.strip()
    df.loc[df["industry"] == "", "industry"] = pd.NA

    # å…ˆåˆ è„æ•°æ®ï¼šæ ¸å¿ƒå­—æ®µç¼ºå¤±ç›´æ¥æ·˜æ±°
    df = df.dropna(subset=["bias_200", "rs_10", "min_vol_ratio_5d", "industry"])
    if df.empty:
        return pd.DataFrame()

    # åŠ¨æ€æ°´æ¸©é˜ˆå€¼
    bias_min, bias_max = _resolve_bias_range(regime)
    df = df[(df["bias_200"] >= bias_min) & (df["bias_200"] <= bias_max)]
    if df.empty:
        return pd.DataFrame()

    # ç™¾åˆ†ä½å› å­åˆ†æ•°
    df["rs_score"] = df["rs_10"].rank(pct=True, ascending=True, method="average")
    # é‡æ¯”è¶Šå°è¶Šå¥½ï¼šascending=False ä½¿å°å€¼è·å¾—æ›´é«˜åˆ†ä½
    df["dry_score"] = df["min_vol_ratio_5d"].rank(
        pct=True, ascending=False, method="average"
    )
    df["base_wyckoff_score"] = 0.6 * df["rs_score"] + 0.4 * df["dry_score"]

    # åŠ¨æ€ä¸»çº¿è¯†åˆ«ï¼šå€™é€‰æ± å†…â€œæœ‰é›†ç¾¤ä¸”ç›¸å¯¹å¼ºåº¦é«˜â€çš„è¡Œä¸š
    industry_stats = (
        df.groupby("industry", as_index=False)
        .agg(stock_count=("code", "count"), avg_rs=("rs_score", "mean"))
    )
    valid_industry_stats = industry_stats[
        industry_stats["stock_count"] >= DYNAMIC_MAINLINE_MIN_CLUSTER
    ]
    hot_industries: set[str] = set()
    if not valid_industry_stats.empty:
        hot_industries = set(
            valid_industry_stats.nlargest(DYNAMIC_MAINLINE_TOP_N, "avg_rs")["industry"]
            .astype(str)
            .tolist()
        )
    df["is_hot_mainline"] = df["industry"].astype(str).isin(hot_industries)
    df["policy_tag"] = df.apply(
        lambda r: _format_mainline_tag(str(r.get("industry", "")), bool(r.get("is_hot_mainline"))),
        axis=1,
    )
    df["dynamic_bonus"] = df["is_hot_mainline"].map(
        lambda v: float(bonus_rate) if bool(v) else 0.0
    )
    df["wyckoff_score"] = df["base_wyckoff_score"] * (1.0 + df["dynamic_bonus"])

    # å…ˆå…¨å±€æ’åºï¼Œå†åšè¡Œä¸šæ‹¥æŒ¤åº¦é™åˆ¶
    df = df.sort_values("wyckoff_score", ascending=False).copy()
    df["industry_rank"] = (
        df.groupby("industry")["wyckoff_score"]
        .rank(ascending=False, method="first")
        .astype(int)
    )
    df = df.groupby("industry", group_keys=False).head(max_per_industry)
    df = df.head(max_total).reset_index(drop=True)
    if hot_industries:
        print(f"[step3] åŠ¨æ€ä¸»çº¿è¡Œä¸š: {', '.join(sorted(hot_industries))}")
    else:
        print("[step3] åŠ¨æ€ä¸»çº¿è¡Œä¸š: æ— ï¼ˆæœªå½¢æˆæœ‰æ•ˆè¡Œä¸šé›†ç¾¤ï¼‰")
    return df


def generate_stock_payload(
    stock_code: str,
    stock_name: str,
    wyckoff_tag: str,
    df: pd.DataFrame,
    *,
    industry: str | None = None,
    quant_score: float | None = None,
    industry_rank: int | None = None,
    policy_tag: str | None = None,
) -> str:
    """
    ç¬¬äº”æ­¥ï¼šå°† 500 å¤© OHLCV æµ“ç¼©ä¸ºå‘ç»™ AI çš„é«˜å¯†åº¦æ–‡æœ¬ã€‚
    1. å¤§èƒŒæ™¯ï¼ˆMA50 / MA200 / ä¹–ç¦»ç‡ï¼‰
    2. è¿‘ 15 æ—¥é‡ä»·åˆ‡ç‰‡ï¼ˆæ”¾é‡æ¯” + æ¶¨è·Œå¹…ï¼‰
    3. è¿‘ 60 æ—¥å¼‚åŠ¨é«˜å…‰æ—¶åˆ»
    """
    df = df.copy().sort_values("date").reset_index(drop=True)
    close = df["close"].astype(float)
    volume = df["volume"].astype(float)
    df["ma50"] = close.rolling(50).mean()
    df["ma200"] = close.rolling(200).mean()
    df["vol_ma20"] = volume.rolling(20).mean()
    df["pct_chg_calc"] = close.pct_change() * 100

    latest = df.iloc[-1]
    ma50_val = latest["ma50"]
    ma200_val = latest["ma200"]
    close_val = latest["close"]

    if pd.notna(ma50_val) and pd.notna(ma200_val) and ma200_val > 0:
        if ma50_val > ma200_val:
            trend = "é•¿æœŸå¤šå¤´æ’åˆ— (MA50 > MA200)"
        else:
            trend = "é•¿æœŸç©ºå¤´æˆ–éœ‡è¡ (MA50 <= MA200)"
        bias_200 = (close_val - ma200_val) / ma200_val * 100
        background = (
            f"  [ç»“æ„èƒŒæ™¯] ç°ä»·:{close_val:.2f}, MA50:{ma50_val:.2f}, MA200:{ma200_val:.2f}ã€‚"
            f"{trend}ï¼Œå¹´çº¿ä¹–ç¦»ç‡:{bias_200:.1f}%"
        )
    else:
        background = f"  [ç»“æ„èƒŒæ™¯] ç°ä»·:{close_val:.2f}ï¼ˆæ•°æ®ä¸è¶³ä»¥è®¡ç®— MA200ï¼‰"

    policy_prefix = f" {policy_tag}" if policy_tag else ""
    header = (
        f"â€¢ {stock_code} {stock_name}{policy_prefix} | æœºå™¨æ ‡ç­¾ï¼š{wyckoff_tag}\n"
        f"  [ä»·æ ¼é”šç‚¹] æœ€æ–°å®é™…æ”¶ç›˜ä»·={close_val:.2f}ï¼ˆæ‰§è¡Œå»ºè®®éœ€å›´ç»•è¯¥é”šç‚¹ç»™å‡ºç»“æ„æˆ˜åŒºï¼Œä¸å¾—ç»™å•ç‚¹é¢„æµ‹ä»·ï¼‰ã€‚\n"
        f"{background}\n"
    )
    if industry:
        header += f"  [è¡Œä¸š] {industry}\n"
    if quant_score is not None:
        rank_text = f"ï¼Œè¡Œä¸šå†…æ’å Top {industry_rank}" if industry_rank is not None else ""
        header += f"  [é‡åŒ–è¯„åˆ†] ç»¼åˆäººå› å­å¾—åˆ†: {quant_score:.3f}{rank_text}\n"

    # è¿‘ 15 æ—¥é‡ä»·åˆ‡ç‰‡
    recent = df.tail(RECENT_DAYS)
    recent_lines = ["  [è¿‘15æ—¥é‡ä»·åˆ‡ç‰‡]:"]
    for _, row in recent.iterrows():
        vol_ratio = row["volume"] / row["vol_ma20"] if pd.notna(row["vol_ma20"]) and row["vol_ma20"] > 0 else 0
        pct = row["pct_chg_calc"] if pd.notna(row["pct_chg_calc"]) else 0
        date_str = str(row["date"])[5:10]
        recent_lines.append(f"    {date_str}: æ”¶{row['close']:.2f} ({pct:+.1f}%), é‡æ¯”:{vol_ratio:.1f}x")

    # è¿‘ 60 æ—¥å¼‚åŠ¨é«˜å…‰
    tail60 = df.tail(HIGHLIGHT_DAYS)
    highlights = []
    for _, row in tail60.iterrows():
        pct = row["pct_chg_calc"] if pd.notna(row["pct_chg_calc"]) else 0
        vol_ratio = row["volume"] / row["vol_ma20"] if pd.notna(row["vol_ma20"]) and row["vol_ma20"] > 0 else 0
        if abs(pct) >= HIGHLIGHT_PCT_THRESHOLD or vol_ratio >= HIGHLIGHT_VOL_RATIO:
            date_str = str(row["date"])[5:10]
            tag_parts = []
            if abs(pct) >= HIGHLIGHT_PCT_THRESHOLD:
                tag_parts.append(f"æ¶¨è·Œ{pct:+.1f}%")
            if vol_ratio >= HIGHLIGHT_VOL_RATIO:
                tag_parts.append(f"é‡æ¯”{vol_ratio:.1f}x")
            highlights.append(f"    {date_str}: æ”¶{row['close']:.2f} ({', '.join(tag_parts)})")

    highlight_section = ""
    if highlights:
        highlight_section = "\n  [è¿‘60æ—¥å¼‚åŠ¨é«˜å…‰]:\n" + "\n".join(highlights) + "\n"

    return header + "\n".join(recent_lines) + "\n" + highlight_section + "\n"


def run(
    symbols_info: list[dict] | list[str],
    webhook_url: str,
    api_key: str,
    model: str,
    benchmark_context: dict | None = None,
) -> tuple[bool, str, str]:
    """
    æ‹‰å– OHLCV â†’ ç¬¬äº”æ­¥ç‰¹å¾å·¥ç¨‹ â†’ AI ç ”æŠ¥ â†’ é£ä¹¦å‘é€ã€‚
    symbols_info: list[{"code", "name", "tag"}] æˆ– list[str]ï¼ˆå‘åå…¼å®¹ï¼‰ã€‚
    """
    if not symbols_info:
        print("[step3] æ— è¾“å…¥è‚¡ç¥¨ï¼Œè·³è¿‡")
        return (True, "skipped_no_symbols", "")

    # å…¼å®¹æ—§è°ƒç”¨ï¼ˆçº¯ str åˆ—è¡¨ï¼‰
    items: list[dict] = []
    for s in symbols_info:
        if isinstance(s, str):
            items.append({"code": s, "name": s, "tag": ""})
        else:
            items.append(s)

    print(f"[step3] AI è¾“å…¥è‚¡ç¥¨æ•°={len(items)}ï¼ˆå…¨é‡å‘½ä¸­è¾“å…¥ï¼‰")

    end_day = _job_end_calendar_day()
    window = _resolve_trading_window(end_calendar_day=end_day, trading_days=TRADING_DAYS)

    regime = (benchmark_context or {}).get("regime", "NEUTRAL")
    sector_map = fetch_sector_map()
    benchmark_ret_10: float | None = None
    try:
        bench_df = fetch_index_hist("000001", window.start_trade_date, window.end_trade_date)
        benchmark_ret_10 = _safe_return(bench_df["close"], lookback=10)
    except Exception:
        benchmark_ret_10 = None

    parts: list[str] = []
    failed: list[tuple[str, str]] = []
    candidate_rows: list[dict] = []
    code_to_df: dict[str, pd.DataFrame] = {}
    for item in items:
        code = item["code"]
        name = item.get("name", code)
        tag = item.get("tag", "")
        try:
            df_raw = _fetch_hist(code, window, "qfq")
            df = normalize_hist_from_fetch(df_raw)
            if ENFORCE_TARGET_TRADE_DATE:
                latest_trade_date = _latest_trade_date_from_hist(df)
                if latest_trade_date != window.end_trade_date:
                    df, patched = _append_spot_bar_if_needed(
                        code,
                        df,
                        window.end_trade_date,
                    )
                    if patched:
                        latest_trade_date = _latest_trade_date_from_hist(df)
                        print(f"[step3] {code} å®æ—¶å¿«ç…§è¡¥å¿æˆåŠŸ")
                if latest_trade_date != window.end_trade_date:
                    failed.append(
                        (
                            code,
                            f"latest_trade_date={latest_trade_date}, target_trade_date={window.end_trade_date}",
                        )
                    )
                    continue
            code_to_df[code] = df

            close = pd.to_numeric(df["close"], errors="coerce")
            volume = pd.to_numeric(df["volume"], errors="coerce")
            ma200 = close.rolling(200).mean()
            latest_close = close.iloc[-1] if len(close) else pd.NA
            latest_ma200 = ma200.iloc[-1] if len(ma200) else pd.NA
            bias_200 = pd.NA
            if pd.notna(latest_close) and pd.notna(latest_ma200) and float(latest_ma200) != 0:
                bias_200 = (float(latest_close) - float(latest_ma200)) / float(latest_ma200) * 100.0

            stock_ret_10 = _safe_return(close, lookback=10)
            rs_10 = stock_ret_10
            if stock_ret_10 is not None and benchmark_ret_10 is not None:
                rs_10 = stock_ret_10 - benchmark_ret_10

            vol_ma20 = volume.rolling(20).mean()
            vol_ratio = volume / vol_ma20.replace(0, pd.NA)
            min_vol_ratio_5d = pd.to_numeric(vol_ratio.tail(5), errors="coerce").min()

            candidate_rows.append(
                {
                    "code": code,
                    "name": name,
                    "tag": tag,
                    "industry": sector_map.get(code, "æœªçŸ¥è¡Œä¸š"),
                    "bias_200": bias_200,
                    "rs_10": rs_10,
                    "min_vol_ratio_5d": min_vol_ratio_5d,
                }
            )
        except Exception as e:
            failed.append((code, str(e)))

    if not candidate_rows:
        if failed:
            detail = ", ".join(f"{s}({e})" for s, e in failed)
            print(f"[step3] OHLCV å…¨éƒ¨æ‹‰å–å¤±è´¥: {detail}")
            return (False, "data_all_failed", "")
        return (True, "no_data_but_no_error", "")

    candidates_df = pd.DataFrame(candidate_rows)
    candidates_df["code"] = candidates_df["code"].astype(str).str.strip()
    candidates_df["policy_tag"] = ""
    selected_df = candidates_df.copy()
    selected_df["wyckoff_score"] = pd.NA
    selected_df["industry_rank"] = pd.NA

    if STEP3_ENABLE_COMPRESSION:
        compressed_df = ultimate_compressor(
            candidates_df,
            regime=regime,
            bonus_rate=DYNAMIC_MAINLINE_BONUS_RATE,
            max_total=STEP3_MAX_AI_INPUT,
            max_per_industry=STEP3_MAX_PER_INDUSTRY,
        )
        if compressed_df.empty:
            print("[step3] å‹ç¼©å™¨ç»“æœä¸ºç©ºï¼Œå›é€€ä¸ºå…¨é‡å€™é€‰åˆ—è¡¨")
        else:
            selected_df = compressed_df
        print(
            f"[step3] å€™é€‰å‹ç¼©å·²å¯ç”¨: raw={len(candidates_df)} -> selected={len(selected_df)} "
            f"(regime={regime}, max_total={STEP3_MAX_AI_INPUT}, max_per_industry={STEP3_MAX_PER_INDUSTRY})"
        )
    else:
        print(f"[step3] å€™é€‰å‹ç¼©æœªå¯ç”¨: selected=å…¨é‡{len(selected_df)}")

    if len(selected_df) > STEP3_MAX_AI_INPUT:
        before_n = len(selected_df)
        selected_df = selected_df.head(STEP3_MAX_AI_INPUT).reset_index(drop=True)
        print(
            f"[step3] ä¸Šä¸‹æ–‡ç¡¬ä¸Šé™ç”Ÿæ•ˆ: selected {before_n} -> {len(selected_df)} "
            f"(STEP3_MAX_AI_INPUT={STEP3_MAX_AI_INPUT})"
        )

    # P2: RAG é˜²é›·ï¼ˆè´Ÿé¢æ–°é—»å…³é”®è¯ vetoï¼‰
    rag_veto_lines: list[str] = []
    if STEP3_ENABLE_RAG_VETO and is_rag_veto_enabled() and not selected_df.empty:
        rag_inputs = [
            {"code": str(r.get("code", "")).strip(), "name": str(r.get("name", ""))}
            for _, r in selected_df.iterrows()
        ]
        veto_map = run_negative_news_veto(rag_inputs)
        vetoed_codes: list[str] = []
        for code, result in veto_map.items():
            if result.error:
                print(f"[step3][rag] {code} æ£€ç´¢å¼‚å¸¸: {result.error}")
            if result.veto:
                vetoed_codes.append(code)
                hit_text = "ã€".join(result.hits[:5]) if result.hits else "è´Ÿé¢å…³é”®è¯"
                ev_text = f" | è¯æ®: {result.evidence[0]}" if result.evidence else ""
                rag_veto_lines.append(f"- {code} {result.name}: å‘½ä¸­ {hit_text}{ev_text}")
        if vetoed_codes:
            before_n = len(selected_df)
            selected_df = selected_df[~selected_df["code"].astype(str).isin(set(vetoed_codes))].reset_index(drop=True)
            print(f"[step3][rag] è´Ÿé¢æ–°é—» veto: {before_n} -> {len(selected_df)}ï¼ˆå‰”é™¤{len(vetoed_codes)}ï¼‰")
        else:
            print("[step3][rag] æœªå‘½ä¸­è´Ÿé¢å…³é”®è¯ï¼Œä¿æŒå€™é€‰ä¸å˜")
    else:
        if STEP3_ENABLE_RAG_VETO:
            print("[step3][rag] æœªå¯ç”¨ï¼ˆç¼ºå°‘ TAVILY_API_KEY/SERPAPI_API_KEY æˆ–å€™é€‰ä¸ºç©ºï¼‰")

    selected_codes = [str(x) for x in selected_df["code"].tolist()]
    if not selected_codes:
        report = (
            "# ğŸ›ï¸ Alpha æŠ•å§”ä¼šæœºå¯†ç”µæŠ¥ï¼šä»Šæ—¥æœ€ç»ˆå†³æ–­\n\n"
            "## ğŸ“š è§‚å¯Ÿæ± ï¼ˆæ•°é‡ä¸é™ï¼‰\n"
            "- æ— ï¼ˆå€™é€‰å‡è¢« RAG é˜²é›· veto æˆ–æ•°æ®ä¸è¶³ï¼‰\n\n"
            f"## âš”ï¸ å¯æ“ä½œæ± ï¼ˆå›ºå®š {OPERATION_TARGET} åªï¼‰\n"
            "- æ— ï¼ˆé£é™©è¿‡é«˜ï¼Œä»Šæ—¥è§‚æœ›ï¼‰"
        )
        if rag_veto_lines:
            report += "\n\n## ğŸ›‘ RAG é˜²é›·å‰”é™¤æ¸…å•\n" + "\n".join(rag_veto_lines)
        model_banner = f"ğŸ¤– æ¨¡å‹: {model}"
        content = f"{model_banner}\n\n{report}"
        title = f"ğŸ“„ æ‰¹é‡ç ”æŠ¥ {date.today().strftime('%Y-%m-%d')}"
        sent = send_feishu_notification(webhook_url, title, content)
        if not sent:
            return (False, "feishu_failed", report)
        return (True, "ok", report)

    for _, row in selected_df.iterrows():
        code = str(row["code"])
        df = code_to_df.get(code)
        if df is None:
            continue
        policy_val = row.get("policy_tag")
        policy_text = (
            str(policy_val).strip()
            if isinstance(policy_val, str) and str(policy_val).strip()
            else None
        )
        payload = generate_stock_payload(
            stock_code=code,
            stock_name=str(row.get("name", code)),
            wyckoff_tag=str(row.get("tag", "")),
            df=df,
            industry=str(row.get("industry", "")),
            quant_score=float(row["wyckoff_score"]) if pd.notna(row.get("wyckoff_score")) else None,
            industry_rank=int(row["industry_rank"]) if pd.notna(row.get("industry_rank")) else None,
            policy_tag=policy_text,
        )
        parts.append(payload)

    benchmark_lines = []
    if benchmark_context:
        benchmark_lines.append("[å®è§‚æ°´æ¸© / Benchmark Context]")
        benchmark_lines.append(
            f"regime={benchmark_context.get('regime')}, "
            f"close={benchmark_context.get('close')}, "
            f"ma50={benchmark_context.get('ma50')}, "
            f"ma200={benchmark_context.get('ma200')}, "
            f"ma50_slope_5d={benchmark_context.get('ma50_slope_5d')}"
        )
        benchmark_lines.append(
            f"recent3_pct={benchmark_context.get('recent3_pct')}, "
            f"recent3_cum_pct={benchmark_context.get('recent3_cum_pct')}, "
            f"tuned={benchmark_context.get('tuned')}"
        )

    user_message = (
        ("{}\n\n".format("\n".join(benchmark_lines)) if benchmark_lines else "")
        + (
            (
                f"[é‡åŒ–å‹ç¼©] å€™é€‰å·²ä» {len(candidates_df)} åªå‹ç¼©åˆ° {len(parts)} åªï¼Œ"
                f"regime={regime}, max_total={STEP3_MAX_AI_INPUT}, "
                f"max_per_industry={STEP3_MAX_PER_INDUSTRY}ã€‚\n\n"
            )
            if STEP3_ENABLE_COMPRESSION and len(candidates_df) > len(parts)
            else ""
        )
        + (
            "ä»¥ä¸‹æ˜¯é€šè¿‡ Wyckoff Funnel å‘½ä¸­å¹¶ç»é‡åŒ–å‹ç¼©åçš„å€™é€‰åå•ã€‚\n"
            if STEP3_ENABLE_COMPRESSION
            else "ä»¥ä¸‹æ˜¯é€šè¿‡ Wyckoff Funnel å‘½ä¸­çš„å…¨é‡å€™é€‰åå•ï¼ˆæœªå‹ç¼©ï¼‰ã€‚\n"
        )
        + "è¯·å…ˆä»å…¨éƒ¨è¾“å…¥ä¸­ç­›å‡ºâ€œå€¼å¾—åŠ å…¥è‡ªé€‰è§‚å¯Ÿæ± â€çš„æ ‡çš„ï¼ˆæ•°é‡ä¸é™ï¼‰ï¼Œå¹¶æ˜ç¡®æ¯åªçš„è§‚å¯Ÿæ¡ä»¶ï¼›"
        + f"å†ä»è§‚å¯Ÿæ± ä¸­ä¸¥æ ¼æŒ‘é€‰â€œæ¬¡æ—¥å¯ä¹°å…¥çš„æ“ä½œæ± â€{OPERATION_TARGET}åªã€‚\n"
        + f"è¾“å‡ºå¿…é¡»åŒ…å«ä¸¤ä¸ªéƒ¨åˆ†ï¼š1) è§‚å¯Ÿæ± ï¼ˆä¸é™ï¼Œå«è§‚å¯Ÿæ¡ä»¶ï¼‰ 2) æ“ä½œæ± ï¼ˆå›ºå®š{OPERATION_TARGET}åªï¼‰ã€‚\n"
        + "ç¡¬çº¦æŸï¼šæ“ä½œæ± å¿…é¡»æ˜¯è§‚å¯Ÿæ± å­é›†ï¼Œä¸”ä¸¤éƒ¨åˆ†åªèƒ½ä½¿ç”¨è¾“å…¥åˆ—è¡¨ä¸­çš„è‚¡ç¥¨ä»£ç ã€‚\n\n"
        + "äº¤æ˜“æ‰§è¡Œç¡¬çº¦æŸï¼š\n"
        + "1) ç¦æ­¢å•ç‚¹ä»·æ ¼æŒ‡ä»¤ï¼Œå¿…é¡»ç»™â€œç»“æ„æˆ˜åŒº(Action Zone) + ç›˜é¢ç¡®è®¤æ¡ä»¶(Tape Condition)â€ã€‚\n"
        + "2) æˆ˜åŒºéœ€å›´ç»•æ¯åªè‚¡ç¥¨çš„â€œä»·æ ¼é”šç‚¹ï¼ˆæœ€æ–°æ”¶ç›˜ä»·ï¼‰â€æè¿°ï¼Œä½†ä¸å¾—åˆ»èˆŸæ±‚å‰‘ã€‚\n"
        + "3) ä¹°å…¥è§¦å‘å¿…é¡»åŒ…å«é‡ä»·ç¡®è®¤æ¡ä»¶ï¼ˆå¦‚ç¼©é‡å›è¸©/æ‹’ç»ä¸‹ç ´ï¼‰ï¼›è‹¥æ”¾é‡ä¸‹ç ´ï¼Œå¿…é¡»å–æ¶ˆä¹°å…¥ã€‚\n"
        + "4) å¼ºåŠ¿çªç ´æ ‡çš„å¿…é¡»ç»™â€œé˜²è¸ç©ºç­–ç•¥â€ï¼šå¼€ç›˜å¼ºåŠ¿ç¡®è®¤åå¯å…ˆç”¨è®¡åˆ’ä»“ä½1/3è¯•å•ï¼Œå…¶ä½™ç­‰å¾…äºŒæ¬¡ç¡®è®¤ã€‚\n\n"
        + (
            "[RAGé˜²é›·å‰”é™¤æ¸…å•]\n"
            + "\n".join(rag_veto_lines)
            + "\n\n"
            if rag_veto_lines
            else ""
        )
        + "\n".join(parts)
    )
    selected_set = set(selected_codes)
    selected_items = [x for x in items if str(x.get("code")) in selected_set]
    _dump_model_input(items=selected_items, model=model, system_prompt=WYCKOFF_FUNNEL_SYSTEM_PROMPT, user_message=user_message)

    report = ""
    used_model = ""
    models_to_try = [model]
    if GEMINI_MODEL_FALLBACK and GEMINI_MODEL_FALLBACK != model:
        models_to_try.append(GEMINI_MODEL_FALLBACK)

    for m in models_to_try:
        try:
            report = call_llm(
                provider="gemini",
                model=m,
                api_key=api_key,
                system_prompt=WYCKOFF_FUNNEL_SYSTEM_PROMPT,
                user_message=user_message,
                timeout=300,
                max_output_tokens=STEP3_MAX_OUTPUT_TOKENS,
            )
            used_model = m
            break
        except Exception as e:
            print(f"[step3] æ¨¡å‹ {m} å¤±è´¥: {e}")
            if m == models_to_try[-1]:
                return (False, "llm_failed", "")

    if not _has_required_sections(report):
        print("[step3] é¦–ç‰ˆç ”æŠ¥ç¼ºå°‘è§‚å¯Ÿæ± /å¯æ“ä½œæ± ï¼Œæ‰§è¡Œä¸€æ¬¡ç»“æ„ä¿®å¤")
        report = _repair_report_structure(
            report=report,
            model=used_model or model,
            api_key=api_key,
            selected_codes=selected_codes,
        )
    if not _has_required_sections(report):
        print("[step3] ç»“æ„ä¿®å¤åä»ç¼ºå°‘å…³é”®ç« èŠ‚ï¼Œè¿½åŠ ç³»ç»Ÿå…œåº•åˆ†å±‚")
        report = report.rstrip() + "\n\n" + _build_fallback_sections(selected_df)

    model_banner = f"ğŸ¤– æ¨¡å‹: {used_model or model}"
    code_name = {
        str(row.get("code")): str(row.get("name", row.get("code")))
        for _, row in selected_df.iterrows()
    }
    selected_set = set(selected_codes)
    # ä¼˜å…ˆç›´æ¥ JSON è§£æï¼›ä¸è¶³æ—¶æ­£åˆ™æ‰«æ–‡æœ¬ï¼›æœ€åä»å€™é€‰åˆ—è¡¨è¡¥é½ã€‚
    # ä¸å†å‘èµ·ç¬¬äºŒæ¬¡ LLM è°ƒç”¨ï¼Œé¿å…å»¶è¿Ÿç¿»å€å’Œ token æµªè´¹ã€‚
    structured = _try_parse_structured_report(
        report=report,
        allowed_codes=selected_set,
        code_name=code_name,
    )
    ops_codes: list[str] = []
    if structured and structured.get("operation_pool"):
        for item in structured["operation_pool"]:
            code = str(item.get("code", "")).strip()
            if code and code not in ops_codes:
                ops_codes.append(code)
    if not ops_codes:
        ops_codes = _extract_codes_from_text(report, selected_set)
    if len(ops_codes) < OPERATION_TARGET:
        for c in selected_codes:
            if c not in ops_codes:
                ops_codes.append(c)
            if len(ops_codes) >= OPERATION_TARGET:
                break
    ops_lines = [f"- {c} {code_name.get(c, c)}" for c in ops_codes[:OPERATION_TARGET]]
    ops_preview = (
        "## âš”ï¸ å¯æ“ä½œæ± é€Ÿè§ˆï¼ˆå‰ç½®ï¼‰\n"
        + ("\n".join(ops_lines) if ops_lines else "- æ— ")
        + "\n\n---\n"
    )

    content = f"{model_banner}\n\n{ops_preview}\n{report}"
    if rag_veto_lines:
        content += "\n\n## ğŸ›‘ RAG é˜²é›·å‰”é™¤æ¸…å•\n" + "\n".join(rag_veto_lines)
    print(f"[step3] é£ä¹¦å‘é€åŸæ–‡é•¿åº¦={len(content)}ï¼ˆä¸å‹ç¼©ï¼Œäº¤ç”±é£ä¹¦åˆ†ç‰‡ï¼‰")
    print(f"[step3] ç ”æŠ¥å®é™…ä½¿ç”¨æ¨¡å‹={used_model or model}")
    if failed:
        content += f"\n\n**è·å–å¤±è´¥**: {', '.join(f'{s}({e})' for s, e in failed)}"

    title = f"ğŸ“„ æ‰¹é‡ç ”æŠ¥ {date.today().strftime('%Y-%m-%d')}"
    sent = send_feishu_notification(webhook_url, title, content)
    if not sent:
        print("[step3] é£ä¹¦æ¨é€å¤±è´¥")
        return (False, "feishu_failed", report)
    print(f"[step3] ç ”æŠ¥å‘é€æˆåŠŸï¼Œè‚¡ç¥¨æ•°={len(parts)}ï¼Œæ‹‰å–å¤±è´¥æ•°={len(failed)}")
    return (True, "ok", report)
